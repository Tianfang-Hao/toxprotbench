ğŸš€ å¼€å§‹å¤„ç†æ¨¡å‹: Qwen/Qwen2.5-7B-Instruct
   - æ¸…ç†åçš„æ¨¡å‹å: Qwen_Qwen2.5-7B-Instruct
   - åˆ›å»ºè¾“å‡ºç›®å½•: results/Qwen_Qwen2.5-7B-Instruct
   - æ­£åœ¨è¿è¡Œ Animal toxin prediction ä»»åŠ¡...
--- å¼€å§‹æ‰§è¡Œæ¨ç†ä»»åŠ¡ ---
  æ¨¡å‹: Qwen/Qwen2.5-7B-Instruct
  è¾“å‡ºç›®å½•: results/Qwen_Qwen2.5-7B-Instruct/Animal
  æ¸©åº¦: 0.0 0.7
W1020 19:17:35.833000 151522 site-packages/torch/distributed/run.py:774] 
W1020 19:17:35.833000 151522 site-packages/torch/distributed/run.py:774] *****************************************
W1020 19:17:35.833000 151522 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1020 19:17:35.833000 151522 site-packages/torch/distributed/run.py:774] *****************************************
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank6]:[W1020 19:17:43.801762091 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank7]:[W1020 19:17:43.063506369 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1020 19:17:43.073394851 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank4]:[W1020 19:17:43.202645694 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1020 19:17:43.277698437 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1020 19:17:43.300293328 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
============================================================
ğŸš€ å¼€å§‹åˆ†å¸ƒå¼æ¨ç†ä»»åŠ¡ï¼Œå…± 8 ä¸ª GPUã€‚
ğŸ“‚ Prompt æ–‡ä»¶: dataset/test.jsonl
ğŸ¤– æ¨¡å‹åˆ—è¡¨: ['Qwen/Qwen2.5-7B-Instruct']
ğŸ”¥ æ¸©åº¦åˆ—è¡¨: [0.0, 0.7]
ğŸ“ è¾“å‡ºç›®å½•: results/Qwen_Qwen2.5-7B-Instruct/Animal
============================================================
æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?it/s]
ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct...
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1020 19:17:43.344378256 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank5]:[W1020 19:17:43.346877775 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.0:   0%|          | 0/2 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 94.91it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 95.09it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 91.81it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 93.74it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 94.96it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 95.86it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 92.98it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 93.56it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

GPU-0 Infer:   0%|          | 0/2 [00:00<?, ?it/s][AThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

GPU-0 Infer:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:22<00:22, 22.67s/it][A
GPU-0 Infer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.45s/it][A
                                                          [A
âœ… ç»“æœå·²ä¿å­˜è‡³: results/Qwen_Qwen2.5-7B-Instruct/Animal/temp-0.0.jsonl
æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:55<00:55, 55.96s/it]æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.7:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:55<00:55, 55.96s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 89.29it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 80.67it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 86.64it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 84.10it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 79.68it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 94.01it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 84.85it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 85.29it/s]

GPU-0 Infer:   0%|          | 0/2 [00:00<?, ?it/s][A
GPU-0 Infer:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  2.15it/s][A
GPU-0 Infer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:22<00:00, 13.42s/it][A
                                                          [A
âœ… ç»“æœå·²ä¿å­˜è‡³: results/Qwen_Qwen2.5-7B-Instruct/Animal/temp-0.7.jsonl
æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:52<00:00, 56.31s/it]æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:52<00:00, 56.26s/it]

ğŸ‰ æ‰€æœ‰æ¨ç†ä»»åŠ¡å®Œæˆï¼
âœ… [Step 1/3] Batch inference completed.
--------------------------------------------------
ğŸ§¹ [Step 2/3] Filtering results...
è¾“å‡ºç›®å½• '/data/nnotaa/align-anything/projects/Bio/benchmark/results/Qwen_Qwen2.5-7B-Instruct/Animal_filtered' å·²å‡†å¤‡å°±ç»ªã€‚

æ­£åœ¨å¤„ç†æ–‡ä»¶: results/Qwen_Qwen2.5-7B-Instruct/Animal/temp-0.0.jsonl
å¤„ç†å®Œæˆ: 'temp-0.0.jsonl'ã€‚
  æ€»å…±å¤„ç† 13 è¡Œï¼Œä¿ç•™äº† 0 è¡Œã€‚

æ­£åœ¨å¤„ç†æ–‡ä»¶: results/Qwen_Qwen2.5-7B-Instruct/Animal/temp-0.7.jsonl
å¤„ç†å®Œæˆ: 'temp-0.7.jsonl'ã€‚
  æ€»å…±å¤„ç† 13 è¡Œï¼Œä¿ç•™äº† 2 è¡Œã€‚

æ‰€æœ‰ .jsonl æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼
âœ… [Step 2/3] Filtering completed. Results are in 'filtered_results/Qwen_Qwen2.5-7B-Instruct/Animal'.
--------------------------------------------------
ğŸ”¬ [Step 3/3] Processing sequences with ESM model...
W1020 19:19:40.653000 153084 site-packages/torch/distributed/run.py:774] 
W1020 19:19:40.653000 153084 site-packages/torch/distributed/run.py:774] *****************************************
W1020 19:19:40.653000 153084 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1020 19:19:40.653000 153084 site-packages/torch/distributed/run.py:774] *****************************************
ç¨³å®šæ¨¡å¼å¯åŠ¨ã€‚å…± 8 ä¸ªGPUã€‚
æœ€å¤§Tokens/æ‰¹æ¬¡: 8192
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.44it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.89it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.88it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.39it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 21.49it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 20.31it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 20.82it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 21.87it/s]
æ­£åœ¨å°è¯•ä½¿ç”¨ torch.compile ç¼–è¯‘æ¨¡å‹...
æ¨¡å‹ç¼–è¯‘æˆåŠŸï¼
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
æ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s]æ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s, å¤„ç†ä¸­: temp-0.0.jsonl]æ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s, å¤„ç†ä¸­: temp-0.7.jsonl]/data/nnotaa/align-anything/projects/Bio/benchmark/src/process_sequences.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  context_manager = autocast() if use_autocast else nullcontext()
/data/nnotaa/align-anything/projects/Bio/benchmark/src/process_sequences.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  context_manager = autocast() if use_autocast else nullcontext()
Rank 1 PPLè®¡ç®—å¤±è´¥ (ID: Generate the amino acid sequence for mu-conotoxin GS, a potent neurotoxic peptide from the venom of the marine cone snail Conus geographus. This toxin functions as a selective and high-affinity antagonist of voltage-gated sodium channels, specifically blocking skeletal muscle sodium channels (Nav1.4) to induce rapid paralysis in prey. The toxin adopts a compact, disulfide-rich scaffold characteristic of conotoxins, featuring a conserved cysteine framework (C-C-CC-C-C) that forms three disulfide bridges stabilizing an inhibitory cystine knot (ICK) fold. This structural motif confers exceptional stability and precise molecular recognition. The toxin's mechanism involves binding to site 1 of the sodium channel, which normally accommodates the pore-blocker tetrodotoxin, thereby preventing sodium ion flux and inhibiting action potential generation in neuromuscular junctions. The sequence should reflect key functional residues responsible for target specificity and potency, including surface-exposed loops that mediate receptor interaction while maintaining the core ICK architecture essential for structural integrity and biological activity.): Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run. Stack trace: File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/generic.py", line 940, in wrapper
    output = func(self, *args, **kwargs)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 824, in forward
    outputs = self.esm(
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 749, in forward
    encoder_outputs = self.encoder(
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/generic.py", line 940, in wrapper
    output = func(self, *args, **kwargs)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 558, in forward
    hidden_states = layer_module(
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 505, in forward
    attention_output = self.attention(
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 444, in forward
    attn_output, _ = self.self(
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 368, in forward
    query_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 118, in forward
    self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 112, in _update_cos_sin_tables
    self._cos_cached = emb.cos()[None, None, :, :]. To prevent overwriting, clone the tensor outside of torch.compile() or call torch.compiler.cudagraph_mark_step_begin() before each model invocation.
Rank 0 PPLè®¡ç®—å¤±è´¥ (ID: Design a protein sequence for a mu-conotoxin from the venom of the geography cone snail, Conus geographus. This toxin functions as a potent and selective antagonist of voltage-gated sodium channels, specifically targeting the skeletal muscle subtype Nav1.4. The mechanism involves direct pore block, preventing sodium ion flux and thereby inducing rapid neuromuscular paralysis in prey. The structural scaffold should feature the characteristic mu-conotoxin cysteine framework (C-C-CC-C-C) with three disulfide bridges that stabilize a compact, globular fold. The mature toxin should be relatively small, approximately 22 amino acids, and exhibit high affinity binding to site 1 of the sodium channel through key residues in its loops. The sequence must include conserved cysteine spacing and the pharmacophore essential for Nav1.4 inhibition, reflecting the toxin's role in the predatory venom arsenal of this piscivorous cone snail species.): Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run. Stack trace: File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/generic.py", line 940, in wrapper
    output = func(self, *args, **kwargs)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 824, in forward
    outputs = self.esm(
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 749, in forward
    encoder_outputs = self.encoder(
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/utils/generic.py", line 940, in wrapper
    output = func(self, *args, **kwargs)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 558, in forward
    hidden_states = layer_module(
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 505, in forward
    attention_output = self.attention(
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 444, in forward
    attn_output, _ = self.self(
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 368, in forward
    query_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 118, in forward
    self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)
  File "/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py", line 112, in _update_cos_sin_tables
    self._cos_cached = emb.cos()[None, None, :, :]. To prevent overwriting, clone the tensor outside of torch.compile() or call torch.compiler.cudagraph_mark_step_begin() before each model invocation.
æ–‡ä»¶æ€»è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:01<00:00, 30.76s/file, å¤„ç†ä¸­: temp-0.7.jsonl]æ–‡ä»¶æ€»è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:01<00:00, 30.76s/file, å¤„ç†ä¸­: temp-0.7.jsonl]

æ‰€æœ‰è¿›ç¨‹è®¡ç®—å®Œæˆï¼Œç­‰å¾…åŒæ­¥...
å¼€å§‹åˆå¹¶ä¸´æ—¶æ–‡ä»¶...
åˆå¹¶æ–‡ä»¶:   0%|          | 0/2 [00:00<?, ?file/s]åˆå¹¶æ–‡ä»¶: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 2053.01file/s]
