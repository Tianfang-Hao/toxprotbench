ğŸš€ å¼€å§‹å¤„ç†æ¨¡å‹: /data/nnotaa/align-anything/projects/Bio/outputs/Qwen2.5-7B-Instruct/slice_5380 (æ ‡ç­¾: local)
   - æ¸…ç†åçš„æ¨¡å‹å: _data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380
   - åˆ›å»ºè¾“å‡ºç›®å½•: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380
   - æ­£åœ¨è¿è¡Œ Bacteria exo prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Bacteria exo prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Bacteria exo prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Bacteria exo prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Bacteria exo prediction ä»»åŠ¡...
--- å¼€å§‹æ‰§è¡Œæ¨ç†ä»»åŠ¡ ---
  æ¨¡å‹è·¯å¾„: /data/nnotaa/align-anything/projects/Bio/outputs/Qwen2.5-7B-Instruct/slice_5380
  æ¨¡å‹æ ‡ç­¾: local
  è¾“å‡ºç›®å½•: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria
  æ¸©åº¦: 0.0 0.7
  æ£€æµ‹åˆ° 'local' æ ‡ç­¾, è°ƒç”¨æœ¬åœ°åˆ†å¸ƒå¼æ¨ç†è„šæœ¬...
W1022 21:34:42.669000 143367 site-packages/torch/distributed/run.py:774] 
W1022 21:34:42.669000 143367 site-packages/torch/distributed/run.py:774] *****************************************
W1022 21:34:42.669000 143367 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 21:34:42.669000 143367 site-packages/torch/distributed/run.py:774] *****************************************
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1022 21:34:49.363689687 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1022 21:34:49.518944204 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1022 21:34:50.773996002 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
============================================================
ğŸš€ å¼€å§‹åˆ†å¸ƒå¼æ¨ç†ä»»åŠ¡ï¼Œå…± 8 ä¸ª GPUã€‚
ğŸ“‚ Prompt æ–‡ä»¶: dataset/test.jsonl
ğŸ¤– æ¨¡å‹åˆ—è¡¨: ['/data/nnotaa/align-anything/projects/Bio/outputs/Qwen2.5-7B-Instruct/slice_5380']
ğŸ”¥ æ¸©åº¦åˆ—è¡¨: [0.0, 0.7]
ğŸ“ è¾“å‡ºç›®å½•: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria
============================================================
æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?it/s]
ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380...
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1022 21:34:50.798493664 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank7]:[W1022 21:34:50.256521632 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W1022 21:34:50.256696866 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank6]:[W1022 21:34:50.291055239 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W1022 21:34:50.291055197 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380, Temp: 0.0:   0%|          | 0/2 [00:01<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!

GPU-0 Infer:   0%|          | 0/2 [00:00<?, ?it/s][A
GPU-0 Infer:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.24it/s][A
GPU-0 Infer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.78it/s][A
                                                          [A
âœ… ç»“æœå·²ä¿å­˜è‡³: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria/temp-0.0.jsonl
æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380, Temp: 0.0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:57<00:57, 57.11s/it]æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380, Temp: 0.7:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:57<00:57, 57.11s/it]
GPU-0 Infer:   0%|          | 0/2 [00:00<?, ?it/s][A
GPU-0 Infer:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:21<00:21, 21.64s/it][A
GPU-0 Infer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:24<00:00, 10.52s/it][A
                                                          [A
âœ… ç»“æœå·²ä¿å­˜è‡³: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria/temp-0.7.jsonl
æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380, Temp: 0.7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:52<00:00, 56.27s/it]æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380, Temp: 0.7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:52<00:00, 56.40s/it]

ğŸ‰ æ‰€æœ‰æ¨ç†ä»»åŠ¡å®Œæˆï¼
âœ… [æ­¥éª¤ 1/3] æ‰¹é‡æ¨ç†å®Œæˆã€‚
--------------------------------------------------
ğŸ§¹ [æ­¥éª¤ 2/3] è¿‡æ»¤ç»“æœ...
è¾“å‡ºç›®å½• '/data/nnotaa/align-anything/projects/Bio/benchmark/results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered' å·²å‡†å¤‡å°±ç»ªã€‚

æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria/temp-0.0.jsonl
å¤„ç†å®Œæˆ: 'temp-0.0.jsonl'ã€‚
  æ€»å…±å¤„ç† 13 è¡Œï¼Œä¿ç•™äº† 5 è¡Œã€‚

æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria/temp-0.7.jsonl
å¤„ç†å®Œæˆ: 'temp-0.7.jsonl'ã€‚
  æ€»å…±å¤„ç† 13 è¡Œï¼Œä¿ç•™äº† 5 è¡Œã€‚

æ‰€æœ‰ .jsonl æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼
âœ… [æ­¥éª¤ 2/3] è¿‡æ»¤å®Œæˆã€‚ç»“æœä½äº 'filtered_results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria'.
--------------------------------------------------
ğŸ”¬ [æ­¥éª¤ 3/3] ä½¿ç”¨ ESM æ¨¡å‹å¤„ç†åºåˆ—...
W1022 21:36:46.570000 146008 site-packages/torch/distributed/run.py:774] 
W1022 21:36:46.570000 146008 site-packages/torch/distributed/run.py:774] *****************************************
W1022 21:36:46.570000 146008 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 21:36:46.570000 146008 site-packages/torch/distributed/run.py:774] *****************************************
é«˜æ€§èƒ½æ¨¡å¼å¯åŠ¨ã€‚å…± 8 ä¸ªGPUã€‚
æœ€å¤§Tokens/æ‰¹æ¬¡: 8192
è‡ªåŠ¨æ··åˆç²¾åº¦(autocast)å·²ã€ç¦ç”¨ã€‘ã€‚
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.61it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.73it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 24.35it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.60it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.48it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 21.55it/s]
Rank 0 æ¨¡å‹åŠ è½½å®Œæˆã€‚
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1022 21:36:57.071175569 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank7]:[W1022 21:36:57.607709928 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 24.05it/s]
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1022 21:36:58.842432012 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.29it/s]
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank6]:[W1022 21:36:58.324324783 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank4]:[W1022 21:36:58.410107376 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank5]:[W1022 21:36:59.779707998 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1022 21:37:00.658186619 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1022 21:37:00.167573980 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
æ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s]æ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s, å¤„ç†ä¸­: temp-0.0.jsonl]
Batches (Rank 0): 0batch [00:00, ?batch/s][A
Batches (Rank 0): 1batch [00:01,  1.34s/batch][A
                                              [Aæ–‡ä»¶æ€»è¿›åº¦:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.34s/file, å¤„ç†ä¸­: temp-0.0.jsonl]æ–‡ä»¶æ€»è¿›åº¦:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.34s/file, å¤„ç†ä¸­: temp-0.7.jsonl]
Batches (Rank 0): 0batch [00:00, ?batch/s][A
Batches (Rank 0): 1batch [00:52, 52.95s/batch][A
                                              [Aæ–‡ä»¶æ€»è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:54<00:00, 31.70s/file, å¤„ç†ä¸­: temp-0.7.jsonl]æ–‡ä»¶æ€»è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:54<00:00, 27.14s/file, å¤„ç†ä¸­: temp-0.7.jsonl]

æ‰€æœ‰è¿›ç¨‹è®¡ç®—å®Œæˆï¼Œç­‰å¾…åŒæ­¥...
å¼€å§‹åˆå¹¶ä¸´æ—¶æ–‡ä»¶...
åˆå¹¶æ–‡ä»¶:   0%|          | 0/2 [00:00<?, ?file/s]åˆå¹¶æ–‡ä»¶: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1355.63file/s]
æ­£åœ¨ç”Ÿæˆæœ€ç»ˆçš„CSVæ‘˜è¦æ–‡ä»¶...
ç”Ÿæˆæ‘˜è¦:   0%|          | 0/2 [00:00<?, ?file/s]ç”Ÿæˆæ‘˜è¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12069.94file/s]
æ‘˜è¦æ–‡ä»¶æˆåŠŸä¿å­˜åˆ°: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/summary_results.csv
âœ… [æ­¥éª¤ 3/3] åºåˆ—å¤„ç†å®Œæˆã€‚
--------------------------------------------------
ğŸ”¬ [Step 4/3] cleaning <|im_end|>...
*** è­¦å‘Šï¼šè„šæœ¬å°†ç›´æ¥è¦†ç›–åŸå§‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿å·²å¤‡ä»½ï¼ ***

å¼€å§‹æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm
ç›®æ ‡å­—æ®µ: assistant
ç§»é™¤æ ‡è®°: <|im_end|>

--- æ­£åœ¨å¤„ç† (å°†è¦†ç›–): results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0.jsonl ---
å¤„ç†å®Œæˆ: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0.jsonl (å·²è¦†ç›–)
  æ€»è¡Œæ•°: 5
  ä¿®æ”¹è¡Œæ•°: 5

--- æ­£åœ¨å¤„ç† (å°†è¦†ç›–): results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7.jsonl ---
å¤„ç†å®Œæˆ: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7.jsonl (å·²è¦†ç›–)
  æ€»è¡Œæ•°: 5
  ä¿®æ”¹è¡Œæ•°: 5

æ‰¹é‡å¤„ç†å®Œæ¯•ï¼Œå…±å¤„ç† 2 ä¸ªæ–‡ä»¶ã€‚
âœ… [Step 4/3] <|im_end|> cleaning completed.
--------------------------------------------------
--- æ¨ç†ä»»åŠ¡ç»“æŸ ---
å¼€å§‹è½¬æ¢æ–‡ä»¶: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0.jsonl -> results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0.fasta

è½¬æ¢å®Œæˆï¼
æˆåŠŸå†™å…¥ 5 ä¸ª FASTA æ¡ç›®ã€‚
å¼€å§‹è½¬æ¢æ–‡ä»¶: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7.jsonl -> results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7.fasta

è½¬æ¢å®Œæˆï¼
æˆåŠŸå†™å…¥ 5 ä¸ª FASTA æ¡ç›®ã€‚
--- æ­¥éª¤ 1: æ­£åœ¨å®‰è£…æ‰€éœ€çš„ Python åº“... ---
Requirement already satisfied: torch in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (2.8.0)
Requirement already satisfied: transformers in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (4.56.1)
Requirement already satisfied: sentencepiece in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (0.2.1)
Requirement already satisfied: h5py in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (3.14.0)
Requirement already satisfied: filelock in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (3.19.1)
Requirement already satisfied: typing-extensions>=4.10.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (4.15.0)
Requirement already satisfied: sympy>=1.13.3 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (1.14.0)
Requirement already satisfied: networkx in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (3.5)
Requirement already satisfied: jinja2 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (2025.3.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.93)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.90)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.90)
Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (9.10.2.21)
Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (11.3.3.83)
Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (10.3.9.90)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (11.7.3.90)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.5.8.93)
Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (0.7.1)
Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (2.27.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.90)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.93)
Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (1.13.1.3)
Requirement already satisfied: triton==3.4.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (3.4.0)
Requirement already satisfied: setuptools>=40.8.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from triton==3.4.0->torch) (78.1.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (0.34.4)
Requirement already satisfied: numpy>=1.17 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (2.2.6)
Requirement already satisfied: packaging>=20.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (25.0)
Requirement already satisfied: pyyaml>=5.1 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (2025.9.1)
Requirement already satisfied: requests in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (2.32.5)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (4.67.1)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)
Requirement already satisfied: charset_normalizer<4,>=2 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from requests->transformers) (3.4.3)
Requirement already satisfied: idna<4,>=2.5 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from requests->transformers) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)

--- æ­¥éª¤ 2: æ­£åœ¨åˆ›å»ºæ‰€æœ‰ç›®å½•... ---
ç›®å½•å·²å‡†å¤‡å°±ç»ªã€‚

--- æ­¥éª¤ 3: æ­£åœ¨æ£€æŸ¥å¹¶ä¸‹è½½æ‰€éœ€æ–‡ä»¶... ---
äºŒçº§ç»“æ„æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚

--- æ­¥éª¤ 4: ç¯å¢ƒå‡†å¤‡å°±ç»ªï¼Œæ­£åœ¨å¯åŠ¨ Python è„šæœ¬... ---
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Using device: cuda
æ£€æµ‹åˆ°è¾“å…¥æ˜¯ä¸€ä¸ªç›®å½•: 'results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm'
æ­£åœ¨åŠ è½½ ProtT5 æ¨¡å‹...
æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚
æ‰¾åˆ° 2 ä¸ª FASTA æ–‡ä»¶ï¼Œå‡†å¤‡å¼€å§‹å¤„ç†...

--- æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0.fasta ---
è¯»å–äº† 5 æ¡åºåˆ—ã€‚
5
æ­£åœ¨ä¿å­˜æ¯ä¸ªè›‹ç™½è´¨çš„åµŒå…¥å‘é‡è‡³: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0_per_protein.h5
æ–‡ä»¶ results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0.fasta å¤„ç†å®Œæ¯•ï¼Œè€—æ—¶ 0.51 ç§’ã€‚

--- æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7.fasta ---
è¯»å–äº† 5 æ¡åºåˆ—ã€‚
5
æ­£åœ¨ä¿å­˜æ¯ä¸ªè›‹ç™½è´¨çš„åµŒå…¥å‘é‡è‡³: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7_per_protein.h5
æ–‡ä»¶ results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7.fasta å¤„ç†å®Œæ¯•ï¼Œè€—æ—¶ 0.26 ç§’ã€‚

--- æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼æ€»è€—æ—¶ 0.77 ç§’ ---

--- æ‰€æœ‰æ“ä½œæ‰§è¡Œå®Œæ¯•ï¼ç»“æœä¿å­˜åœ¨ 'results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm' ç›®å½•ä¸­ã€‚ ---
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
/data/nnotaa/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names
  warnings.warn(
/data/nnotaa/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names
  warnings.warn(

Reading protein ID order from: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0.fasta
Found 5 protein IDs in FASTA file.
Loading embeddings from: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0_per_protein.h5

Running predictions...
Prediction counts: Counter({1: 5})

Saving results to: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0_per_protein.jsonl
Successfully saved 5 predictions.

<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
/data/nnotaa/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names
  warnings.warn(
/data/nnotaa/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names
  warnings.warn(

Reading protein ID order from: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7.fasta
Found 5 protein IDs in FASTA file.
Loading embeddings from: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7_per_protein.h5

Running predictions...
Prediction counts: Counter({1: 5})

Saving results to: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7_per_protein.jsonl
Successfully saved 5 predictions.

å¼€å§‹å¤„ç†...
  - ä» 'dataset/test.jsonl' æå–å­—æ®µ: ['id', 'description', 'sequence', 'taxonomy', 'prompt']
  - ä» 'results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0_per_protein.jsonl' æå–å­—æ®µ: ['prediction', 'probability']

å¤„ç†å®Œæˆï¼å…±åˆå¹¶ 5 è¡Œè®°å½•ã€‚
ç»“æœå·²ä¿å­˜åˆ°: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.0_result.jsonl
å¼€å§‹å¤„ç†...
  - ä» 'dataset/test.jsonl' æå–å­—æ®µ: ['id', 'description', 'sequence', 'taxonomy', 'prompt']
  - ä» 'results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7_per_protein.jsonl' æå–å­—æ®µ: ['prediction', 'probability']

å¤„ç†å®Œæˆï¼å…±åˆå¹¶ 5 è¡Œè®°å½•ã€‚
ç»“æœå·²ä¿å­˜åˆ°: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Bacteria_filtered_esm/temp-0.7_result.jsonl
   - æ­£åœ¨è¿è¡Œ Animal toxin prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Animal toxin prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Animal toxin prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Animal toxin prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Animal toxin prediction ä»»åŠ¡...
--- å¼€å§‹æ‰§è¡Œæ¨ç†ä»»åŠ¡ ---
  æ¨¡å‹è·¯å¾„: /data/nnotaa/align-anything/projects/Bio/outputs/Qwen2.5-7B-Instruct/slice_5380
  æ¨¡å‹æ ‡ç­¾: local
  è¾“å‡ºç›®å½•: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal
  æ¸©åº¦: 0.0 0.7
  æ£€æµ‹åˆ° 'local' æ ‡ç­¾, è°ƒç”¨æœ¬åœ°åˆ†å¸ƒå¼æ¨ç†è„šæœ¬...
W1022 21:39:09.669000 147663 site-packages/torch/distributed/run.py:774] 
W1022 21:39:09.669000 147663 site-packages/torch/distributed/run.py:774] *****************************************
W1022 21:39:09.669000 147663 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 21:39:09.669000 147663 site-packages/torch/distributed/run.py:774] *****************************************
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank6]:[W1022 21:39:16.339379862 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank4]:[W1022 21:39:16.566907219 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1022 21:39:16.600117474 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
============================================================
ğŸš€ å¼€å§‹åˆ†å¸ƒå¼æ¨ç†ä»»åŠ¡ï¼Œå…± 8 ä¸ª GPUã€‚
ğŸ“‚ Prompt æ–‡ä»¶: dataset/test.jsonl
ğŸ¤– æ¨¡å‹åˆ—è¡¨: ['/data/nnotaa/align-anything/projects/Bio/outputs/Qwen2.5-7B-Instruct/slice_5380']
ğŸ”¥ æ¸©åº¦åˆ—è¡¨: [0.0, 0.7]
ğŸ“ è¾“å‡ºç›®å½•: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal
============================================================
æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?it/s]
ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380...
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1022 21:39:16.616525772 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank5]:[W1022 21:39:17.054199994 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank7]:[W1022 21:39:17.138018125 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1022 21:39:17.245559757 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1022 21:39:17.287920408 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380, Temp: 0.0:   0%|          | 0/2 [00:01<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!

GPU-0 Infer:   0%|          | 0/2 [00:00<?, ?it/s][A
GPU-0 Infer:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.23it/s][A
GPU-0 Infer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.77it/s][A
                                                          [A
âœ… ç»“æœå·²ä¿å­˜è‡³: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal/temp-0.0.jsonl
æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380, Temp: 0.0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:57<00:57, 57.15s/it]æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380, Temp: 0.7:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:57<00:57, 57.15s/it]
GPU-0 Infer:   0%|          | 0/2 [00:00<?, ?it/s][A
GPU-0 Infer:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  2.02it/s][A
GPU-0 Infer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.23it/s][A
                                                          [A
âœ… ç»“æœå·²ä¿å­˜è‡³: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal/temp-0.7.jsonl
æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380, Temp: 0.7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:52<00:00, 56.21s/it]æ¨¡å‹: Qwen2.5-7B-Instruct_slice_5380, Temp: 0.7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:52<00:00, 56.35s/it]

ğŸ‰ æ‰€æœ‰æ¨ç†ä»»åŠ¡å®Œæˆï¼
âœ… [æ­¥éª¤ 1/3] æ‰¹é‡æ¨ç†å®Œæˆã€‚
--------------------------------------------------
ğŸ§¹ [æ­¥éª¤ 2/3] è¿‡æ»¤ç»“æœ...
è¾“å‡ºç›®å½• '/data/nnotaa/align-anything/projects/Bio/benchmark/results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered' å·²å‡†å¤‡å°±ç»ªã€‚

æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal/temp-0.0.jsonl
å¤„ç†å®Œæˆ: 'temp-0.0.jsonl'ã€‚
  æ€»å…±å¤„ç† 13 è¡Œï¼Œä¿ç•™äº† 5 è¡Œã€‚

æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal/temp-0.7.jsonl
å¤„ç†å®Œæˆ: 'temp-0.7.jsonl'ã€‚
  æ€»å…±å¤„ç† 13 è¡Œï¼Œä¿ç•™äº† 7 è¡Œã€‚

æ‰€æœ‰ .jsonl æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼
âœ… [æ­¥éª¤ 2/3] è¿‡æ»¤å®Œæˆã€‚ç»“æœä½äº 'filtered_results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal'.
--------------------------------------------------
ğŸ”¬ [æ­¥éª¤ 3/3] ä½¿ç”¨ ESM æ¨¡å‹å¤„ç†åºåˆ—...
W1022 21:41:13.445000 149445 site-packages/torch/distributed/run.py:774] 
W1022 21:41:13.445000 149445 site-packages/torch/distributed/run.py:774] *****************************************
W1022 21:41:13.445000 149445 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 21:41:13.445000 149445 site-packages/torch/distributed/run.py:774] *****************************************
é«˜æ€§èƒ½æ¨¡å¼å¯åŠ¨ã€‚å…± 8 ä¸ªGPUã€‚
æœ€å¤§Tokens/æ‰¹æ¬¡: 8192
è‡ªåŠ¨æ··åˆç²¾åº¦(autocast)å·²ã€ç¦ç”¨ã€‘ã€‚
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.81it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.14it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 24.02it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.98it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.83it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.62it/s]
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank5]:[W1022 21:41:24.487078645 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.59it/s]
Rank 0 æ¨¡å‹åŠ è½½å®Œæˆã€‚
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1022 21:41:25.109598275 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1022 21:41:25.172692543 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.38it/s]
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank4]:[W1022 21:41:25.349014827 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank7]:[W1022 21:41:26.142148067 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1022 21:41:26.424043803 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank6]:[W1022 21:41:27.092864022 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1022 21:41:27.306080184 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
æ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s]æ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s, å¤„ç†ä¸­: temp-0.0.jsonl]
Batches (Rank 0): 0batch [00:00, ?batch/s][A
Batches (Rank 0): 1batch [00:01,  1.37s/batch][A
                                              [Aæ–‡ä»¶æ€»è¿›åº¦:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.37s/file, å¤„ç†ä¸­: temp-0.0.jsonl]æ–‡ä»¶æ€»è¿›åº¦:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.37s/file, å¤„ç†ä¸­: temp-0.7.jsonl]
Batches (Rank 0): 0batch [00:00, ?batch/s][A
Batches (Rank 0): 1batch [00:01,  1.04s/batch][A
                                              [Aæ–‡ä»¶æ€»è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.18s/file, å¤„ç†ä¸­: temp-0.7.jsonl]æ–‡ä»¶æ€»è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.21s/file, å¤„ç†ä¸­: temp-0.7.jsonl]

æ‰€æœ‰è¿›ç¨‹è®¡ç®—å®Œæˆï¼Œç­‰å¾…åŒæ­¥...
å¼€å§‹åˆå¹¶ä¸´æ—¶æ–‡ä»¶...
åˆå¹¶æ–‡ä»¶:   0%|          | 0/2 [00:00<?, ?file/s]åˆå¹¶æ–‡ä»¶: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1278.75file/s]
æ­£åœ¨ç”Ÿæˆæœ€ç»ˆçš„CSVæ‘˜è¦æ–‡ä»¶...
ç”Ÿæˆæ‘˜è¦:   0%|          | 0/2 [00:00<?, ?file/s]ç”Ÿæˆæ‘˜è¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12228.29file/s]
æ‘˜è¦æ–‡ä»¶æˆåŠŸä¿å­˜åˆ°: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/summary_results.csv
âœ… [æ­¥éª¤ 3/3] åºåˆ—å¤„ç†å®Œæˆã€‚
--------------------------------------------------
ğŸ”¬ [Step 4/3] cleaning <|im_end|>...
*** è­¦å‘Šï¼šè„šæœ¬å°†ç›´æ¥è¦†ç›–åŸå§‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿å·²å¤‡ä»½ï¼ ***

å¼€å§‹æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm
ç›®æ ‡å­—æ®µ: assistant
ç§»é™¤æ ‡è®°: <|im_end|>

--- æ­£åœ¨å¤„ç† (å°†è¦†ç›–): results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.0.jsonl ---
å¤„ç†å®Œæˆ: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.0.jsonl (å·²è¦†ç›–)
  æ€»è¡Œæ•°: 5
  ä¿®æ”¹è¡Œæ•°: 5

--- æ­£åœ¨å¤„ç† (å°†è¦†ç›–): results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.7.jsonl ---
å¤„ç†å®Œæˆ: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.7.jsonl (å·²è¦†ç›–)
  æ€»è¡Œæ•°: 7
  ä¿®æ”¹è¡Œæ•°: 7

æ‰¹é‡å¤„ç†å®Œæ¯•ï¼Œå…±å¤„ç† 2 ä¸ªæ–‡ä»¶ã€‚
âœ… [Step 4/3] <|im_end|> cleaning completed.
--------------------------------------------------
å¼€å§‹è½¬æ¢æ–‡ä»¶: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.0.jsonl -> results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.0.fasta

è½¬æ¢å®Œæˆï¼
æˆåŠŸå†™å…¥ 5 ä¸ª FASTA æ¡ç›®ã€‚
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.

##############################################################################
# The program ToxinPred2.0 is developed for predicting Toxin and non toxin #
# protein from their primary sequence, developed by Prof G. P. S. Raghava's group. #
# ############################################################################
Summary of Parameters:
Input File:  results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.0.fasta ; Model:  1 ; Threshold:  0.6
Output File:  results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.0_toxinpred_results.csv ; Display:  2

å¼€å§‹è½¬æ¢æ–‡ä»¶: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.7.jsonl -> results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.7.fasta

è½¬æ¢å®Œæˆï¼
æˆåŠŸå†™å…¥ 7 ä¸ª FASTA æ¡ç›®ã€‚
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.

##############################################################################
# The program ToxinPred2.0 is developed for predicting Toxin and non toxin #
# protein from their primary sequence, developed by Prof G. P. S. Raghava's group. #
# ############################################################################
Summary of Parameters:
Input File:  results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.7.fasta ; Model:  1 ; Threshold:  0.6
Output File:  results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.7_toxinpred_results.csv ; Display:  2

å¼€å§‹å¤„ç†...
  - ä» CSV 'results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.0_toxinpred_results.csv' æå–å­—æ®µ: ['ML_Score', 'Prediction']
  - ä» JSONL 'dataset/test.jsonl' æå–å­—æ®µ: ['id', 'description', 'sequence', 'taxonomy', 'prompt']

å¤„ç†å®Œæˆï¼å…±åˆå¹¶ 5 è¡Œè®°å½•ã€‚
ç»“æœå·²ä¿å­˜åˆ°: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.0_result.jsonl
å¼€å§‹å¤„ç†...
  - ä» CSV 'results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.7_toxinpred_results.csv' æå–å­—æ®µ: ['ML_Score', 'Prediction']
  - ä» JSONL 'dataset/test.jsonl' æå–å­—æ®µ: ['id', 'description', 'sequence', 'taxonomy', 'prompt']

å¤„ç†å®Œæˆï¼å…±åˆå¹¶ 7 è¡Œè®°å½•ã€‚
ç»“æœå·²ä¿å­˜åˆ°: results/2025-10-22_21-34-41/_data_nnotaa_align-anything_projects_Bio_outputs_Qwen2.5-7B-Instruct_slice_5380/Animal_filtered_esm/temp-0.7_result.jsonl
âœ… æ¨¡å‹ /data/nnotaa/align-anything/projects/Bio/outputs/Qwen2.5-7B-Instruct/slice_5380 å¤„ç†å®Œæˆã€‚
-----------------------------------------
ğŸš€ å¼€å§‹å¤„ç†æ¨¡å‹: Qwen/Qwen2.5-7B-Instruct (æ ‡ç­¾: local)
   - æ¸…ç†åçš„æ¨¡å‹å: Qwen_Qwen2.5-7B-Instruct
   - åˆ›å»ºè¾“å‡ºç›®å½•: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct
   - æ­£åœ¨è¿è¡Œ Bacteria exo prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Bacteria exo prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Bacteria exo prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Bacteria exo prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Bacteria exo prediction ä»»åŠ¡...
--- å¼€å§‹æ‰§è¡Œæ¨ç†ä»»åŠ¡ ---
  æ¨¡å‹è·¯å¾„: Qwen/Qwen2.5-7B-Instruct
  æ¨¡å‹æ ‡ç­¾: local
  è¾“å‡ºç›®å½•: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria
  æ¸©åº¦: 0.0 0.7
  æ£€æµ‹åˆ° 'local' æ ‡ç­¾, è°ƒç”¨æœ¬åœ°åˆ†å¸ƒå¼æ¨ç†è„šæœ¬...
W1022 21:43:44.304000 151195 site-packages/torch/distributed/run.py:774] 
W1022 21:43:44.304000 151195 site-packages/torch/distributed/run.py:774] *****************************************
W1022 21:43:44.304000 151195 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 21:43:44.304000 151195 site-packages/torch/distributed/run.py:774] *****************************************
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1022 21:43:51.805888753 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank7]:[W1022 21:43:51.877337589 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank6]:[W1022 21:43:51.098266086 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank4]:[W1022 21:43:51.313021574 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
============================================================
ğŸš€ å¼€å§‹åˆ†å¸ƒå¼æ¨ç†ä»»åŠ¡ï¼Œå…± 8 ä¸ª GPUã€‚
ğŸ“‚ Prompt æ–‡ä»¶: dataset/test.jsonl
ğŸ¤– æ¨¡å‹åˆ—è¡¨: ['Qwen/Qwen2.5-7B-Instruct']
ğŸ”¥ æ¸©åº¦åˆ—è¡¨: [0.0, 0.7]
ğŸ“ è¾“å‡ºç›®å½•: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria
============================================================
æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?it/s]
ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct...
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1022 21:43:52.630911431 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank5]:[W1022 21:43:52.643766596 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1022 21:43:52.689330633 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1022 21:43:52.693141050 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.0:   0%|          | 0/2 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 95.38it/s]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 101.63it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 102.65it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 96.81it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 97.45it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 104.10it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 98.17it/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 103.50it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

GPU-0 Infer:   0%|          | 0/2 [00:00<?, ?it/s][AThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

GPU-0 Infer:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:23<00:23, 23.13s/it][A
GPU-0 Infer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:45<00:00, 22.97s/it][A
                                                          [A
âœ… ç»“æœå·²ä¿å­˜è‡³: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria/temp-0.0.jsonl
æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:54<00:54, 54.42s/it]æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.7:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:54<00:54, 54.42s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 91.26it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 91.70it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 99.26it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 94.39it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][ALoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 94.71it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 94.56it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 97.10it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 95.96it/s]

GPU-0 Infer:   0%|          | 0/2 [00:00<?, ?it/s][A
GPU-0 Infer:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.97it/s][A
GPU-0 Infer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:24<00:00, 14.12s/it][A
                                                          [A
âœ… ç»“æœå·²ä¿å­˜è‡³: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria/temp-0.7.jsonl
æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:47<00:00, 53.92s/it]æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:47<00:00, 54.00s/it]

ğŸ‰ æ‰€æœ‰æ¨ç†ä»»åŠ¡å®Œæˆï¼
âœ… [æ­¥éª¤ 1/3] æ‰¹é‡æ¨ç†å®Œæˆã€‚
--------------------------------------------------
ğŸ§¹ [æ­¥éª¤ 2/3] è¿‡æ»¤ç»“æœ...
è¾“å‡ºç›®å½• '/data/nnotaa/align-anything/projects/Bio/benchmark/results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered' å·²å‡†å¤‡å°±ç»ªã€‚

æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria/temp-0.0.jsonl
å¤„ç†å®Œæˆ: 'temp-0.0.jsonl'ã€‚
  æ€»å…±å¤„ç† 13 è¡Œï¼Œä¿ç•™äº† 0 è¡Œã€‚

æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria/temp-0.7.jsonl
å¤„ç†å®Œæˆ: 'temp-0.7.jsonl'ã€‚
  æ€»å…±å¤„ç† 13 è¡Œï¼Œä¿ç•™äº† 1 è¡Œã€‚

æ‰€æœ‰ .jsonl æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼
âœ… [æ­¥éª¤ 2/3] è¿‡æ»¤å®Œæˆã€‚ç»“æœä½äº 'filtered_results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria'.
--------------------------------------------------
ğŸ”¬ [æ­¥éª¤ 3/3] ä½¿ç”¨ ESM æ¨¡å‹å¤„ç†åºåˆ—...
W1022 21:45:43.895000 152878 site-packages/torch/distributed/run.py:774] 
W1022 21:45:43.895000 152878 site-packages/torch/distributed/run.py:774] *****************************************
W1022 21:45:43.895000 152878 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 21:45:43.895000 152878 site-packages/torch/distributed/run.py:774] *****************************************
é«˜æ€§èƒ½æ¨¡å¼å¯åŠ¨ã€‚å…± 8 ä¸ªGPUã€‚
æœ€å¤§Tokens/æ‰¹æ¬¡: 8192
è‡ªåŠ¨æ··åˆç²¾åº¦(autocast)å·²ã€ç¦ç”¨ã€‘ã€‚
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.68it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.59it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.28it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 24.03it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 24.67it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.02it/s]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  7.11it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  6.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.35it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12.13it/s]
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank6]:[W1022 21:45:55.236091096 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank7]:[W1022 21:45:55.448486367 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Rank 0 æ¨¡å‹åŠ è½½å®Œæˆã€‚
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1022 21:45:56.679796190 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1022 21:45:56.020417024 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1022 21:45:56.088543508 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank4]:[W1022 21:45:56.088654791 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1022 21:45:56.278817352 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank5]:[W1022 21:45:56.280992577 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
æ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s]æ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s, å¤„ç†ä¸­: temp-0.0.jsonl]
Batches (Rank 0): 0batch [00:00, ?batch/s][A
                                          [Aæ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s, å¤„ç†ä¸­: temp-0.7.jsonl]
Batches (Rank 0): 0batch [00:00, ?batch/s][A
Batches (Rank 0): 1batch [00:00,  1.04batch/s][A
                                              [Aæ–‡ä»¶æ€»è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.07file/s, å¤„ç†ä¸­: temp-0.7.jsonl]æ–‡ä»¶æ€»è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.07file/s, å¤„ç†ä¸­: temp-0.7.jsonl]

æ‰€æœ‰è¿›ç¨‹è®¡ç®—å®Œæˆï¼Œç­‰å¾…åŒæ­¥...
å¼€å§‹åˆå¹¶ä¸´æ—¶æ–‡ä»¶...
åˆå¹¶æ–‡ä»¶:   0%|          | 0/2 [00:00<?, ?file/s]åˆå¹¶æ–‡ä»¶: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1607.32file/s]
æ­£åœ¨ç”Ÿæˆæœ€ç»ˆçš„CSVæ‘˜è¦æ–‡ä»¶...
ç”Ÿæˆæ‘˜è¦:   0%|          | 0/2 [00:00<?, ?file/s]ç”Ÿæˆæ‘˜è¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 21675.99file/s]
æ‘˜è¦æ–‡ä»¶æˆåŠŸä¿å­˜åˆ°: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/summary_results.csv
âœ… [æ­¥éª¤ 3/3] åºåˆ—å¤„ç†å®Œæˆã€‚
--------------------------------------------------
ğŸ”¬ [Step 4/3] cleaning <|im_end|>...
*** è­¦å‘Šï¼šè„šæœ¬å°†ç›´æ¥è¦†ç›–åŸå§‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿å·²å¤‡ä»½ï¼ ***

å¼€å§‹æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm
ç›®æ ‡å­—æ®µ: assistant
ç§»é™¤æ ‡è®°: <|im_end|>

--- æ­£åœ¨å¤„ç† (å°†è¦†ç›–): results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0.jsonl ---
å¤„ç†å®Œæˆ: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0.jsonl (å·²è¦†ç›–)
  æ€»è¡Œæ•°: 0
  ä¿®æ”¹è¡Œæ•°: 0

--- æ­£åœ¨å¤„ç† (å°†è¦†ç›–): results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7.jsonl ---
å¤„ç†å®Œæˆ: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7.jsonl (å·²è¦†ç›–)
  æ€»è¡Œæ•°: 1
  ä¿®æ”¹è¡Œæ•°: 1

æ‰¹é‡å¤„ç†å®Œæ¯•ï¼Œå…±å¤„ç† 2 ä¸ªæ–‡ä»¶ã€‚
âœ… [Step 4/3] <|im_end|> cleaning completed.
--------------------------------------------------
--- æ¨ç†ä»»åŠ¡ç»“æŸ ---
å¼€å§‹è½¬æ¢æ–‡ä»¶: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0.jsonl -> results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0.fasta

è½¬æ¢å®Œæˆï¼
æˆåŠŸå†™å…¥ 0 ä¸ª FASTA æ¡ç›®ã€‚
å¼€å§‹è½¬æ¢æ–‡ä»¶: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7.jsonl -> results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7.fasta

è½¬æ¢å®Œæˆï¼
æˆåŠŸå†™å…¥ 1 ä¸ª FASTA æ¡ç›®ã€‚
--- æ­¥éª¤ 1: æ­£åœ¨å®‰è£…æ‰€éœ€çš„ Python åº“... ---
Requirement already satisfied: torch in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (2.8.0)
Requirement already satisfied: transformers in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (4.56.1)
Requirement already satisfied: sentencepiece in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (0.2.1)
Requirement already satisfied: h5py in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (3.14.0)
Requirement already satisfied: filelock in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (3.19.1)
Requirement already satisfied: typing-extensions>=4.10.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (4.15.0)
Requirement already satisfied: sympy>=1.13.3 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (1.14.0)
Requirement already satisfied: networkx in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (3.5)
Requirement already satisfied: jinja2 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (2025.3.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.93)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.90)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.90)
Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (9.10.2.21)
Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (11.3.3.83)
Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (10.3.9.90)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (11.7.3.90)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.5.8.93)
Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (0.7.1)
Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (2.27.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.90)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (12.8.93)
Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (1.13.1.3)
Requirement already satisfied: triton==3.4.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from torch) (3.4.0)
Requirement already satisfied: setuptools>=40.8.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from triton==3.4.0->torch) (78.1.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (0.34.4)
Requirement already satisfied: numpy>=1.17 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (2.2.6)
Requirement already satisfied: packaging>=20.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (25.0)
Requirement already satisfied: pyyaml>=5.1 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (2025.9.1)
Requirement already satisfied: requests in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (2.32.5)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (0.22.0)
Requirement already satisfied: safetensors>=0.4.3 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from transformers) (4.67.1)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)
Requirement already satisfied: charset_normalizer<4,>=2 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from requests->transformers) (3.4.3)
Requirement already satisfied: idna<4,>=2.5 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from requests->transformers) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from requests->transformers) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in /data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)

--- æ­¥éª¤ 2: æ­£åœ¨åˆ›å»ºæ‰€æœ‰ç›®å½•... ---
ç›®å½•å·²å‡†å¤‡å°±ç»ªã€‚

--- æ­¥éª¤ 3: æ­£åœ¨æ£€æŸ¥å¹¶ä¸‹è½½æ‰€éœ€æ–‡ä»¶... ---
äºŒçº§ç»“æ„æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚

--- æ­¥éª¤ 4: ç¯å¢ƒå‡†å¤‡å°±ç»ªï¼Œæ­£åœ¨å¯åŠ¨ Python è„šæœ¬... ---
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Using device: cuda
æ£€æµ‹åˆ°è¾“å…¥æ˜¯ä¸€ä¸ªç›®å½•: 'results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm'
æ­£åœ¨åŠ è½½ ProtT5 æ¨¡å‹...
æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚
æ‰¾åˆ° 2 ä¸ª FASTA æ–‡ä»¶ï¼Œå‡†å¤‡å¼€å§‹å¤„ç†...

--- æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0.fasta ---
æ–‡ä»¶ results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0.fasta ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œå·²è·³è¿‡ã€‚

--- æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7.fasta ---
è¯»å–äº† 1 æ¡åºåˆ—ã€‚
1
æ­£åœ¨ä¿å­˜æ¯ä¸ªè›‹ç™½è´¨çš„åµŒå…¥å‘é‡è‡³: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7_per_protein.h5
æ–‡ä»¶ results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7.fasta å¤„ç†å®Œæ¯•ï¼Œè€—æ—¶ 0.16 ç§’ã€‚

--- æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼æ€»è€—æ—¶ 0.16 ç§’ ---

--- æ‰€æœ‰æ“ä½œæ‰§è¡Œå®Œæ¯•ï¼ç»“æœä¿å­˜åœ¨ 'results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm' ç›®å½•ä¸­ã€‚ ---
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.

ERROR conda.cli.main_run:execute(127): `conda run python Bacteria/src/classifying_unknown_proteins.py Bacteria/Predictor/sklearn_svcPC20_SST30_CV10_embeddingsProtT5 results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0_per_protein.h5 results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0.fasta` failed. (See above for error)
Reading protein ID order from: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0.fasta
Error: No protein IDs found in the FASTA file 'results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0.fasta'. Please check the file format.

<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
/data/nnotaa/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names
  warnings.warn(
/data/nnotaa/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names
  warnings.warn(

Reading protein ID order from: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7.fasta
Found 1 protein IDs in FASTA file.
Loading embeddings from: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7_per_protein.h5

Running predictions...
Prediction counts: Counter({1: 1})

Saving results to: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7_per_protein.jsonl
Successfully saved 1 predictions.

å¼€å§‹å¤„ç†...
  - ä» 'dataset/test.jsonl' æå–å­—æ®µ: ['id', 'description', 'sequence', 'taxonomy', 'prompt']
  - ä» 'results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0_per_protein.jsonl' æå–å­—æ®µ: ['prediction', 'probability']
é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ° - [Errno 2] No such file or directory: 'results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.0_per_protein.jsonl'
å¼€å§‹å¤„ç†...
  - ä» 'dataset/test.jsonl' æå–å­—æ®µ: ['id', 'description', 'sequence', 'taxonomy', 'prompt']
  - ä» 'results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7_per_protein.jsonl' æå–å­—æ®µ: ['prediction', 'probability']

å¤„ç†å®Œæˆï¼å…±åˆå¹¶ 1 è¡Œè®°å½•ã€‚
ç»“æœå·²ä¿å­˜åˆ°: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Bacteria_filtered_esm/temp-0.7_result.jsonl
   - æ­£åœ¨è¿è¡Œ Animal toxin prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Animal toxin prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Animal toxin prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Animal toxin prediction ä»»åŠ¡...
   - æ­£åœ¨è¿è¡Œ Animal toxin prediction ä»»åŠ¡...
--- å¼€å§‹æ‰§è¡Œæ¨ç†ä»»åŠ¡ ---
  æ¨¡å‹è·¯å¾„: Qwen/Qwen2.5-7B-Instruct
  æ¨¡å‹æ ‡ç­¾: local
  è¾“å‡ºç›®å½•: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal
  æ¸©åº¦: 0.0 0.7
  æ£€æµ‹åˆ° 'local' æ ‡ç­¾, è°ƒç”¨æœ¬åœ°åˆ†å¸ƒå¼æ¨ç†è„šæœ¬...
W1022 21:46:18.362000 154124 site-packages/torch/distributed/run.py:774] 
W1022 21:46:18.362000 154124 site-packages/torch/distributed/run.py:774] *****************************************
W1022 21:46:18.362000 154124 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 21:46:18.362000 154124 site-packages/torch/distributed/run.py:774] *****************************************
============================================================
ğŸš€ å¼€å§‹åˆ†å¸ƒå¼æ¨ç†ä»»åŠ¡ï¼Œå…± 8 ä¸ª GPUã€‚
ğŸ“‚ Prompt æ–‡ä»¶: dataset/test.jsonl
ğŸ¤– æ¨¡å‹åˆ—è¡¨: ['Qwen/Qwen2.5-7B-Instruct']
ğŸ”¥ æ¸©åº¦åˆ—è¡¨: [0.0, 0.7]
ğŸ“ è¾“å‡ºç›®å½•: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal
============================================================
æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?it/s]
ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct...
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1022 21:46:25.959474324 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank6]:[W1022 21:46:26.729536600 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W1022 21:46:26.729562657 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1022 21:46:26.799700713 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1022 21:46:26.813594605 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank5]:[W1022 21:46:26.842549901 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank7]:[W1022 21:46:26.973082305 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank4]:[W1022 21:46:26.993215117 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.0:   0%|          | 0/2 [00:01<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 99.14it/s]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 102.18it/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 103.36it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 104.38it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 96.14it/s]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 101.93it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 101.83it/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 100.09it/s]

GPU-0 Infer:   0%|          | 0/2 [00:00<?, ?it/s][AThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

GPU-0 Infer:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:21<00:21, 21.91s/it][A
GPU-0 Infer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:43<00:00, 21.60s/it][A
                                                          [A
âœ… ç»“æœå·²ä¿å­˜è‡³: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal/temp-0.0.jsonl
æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:52<00:52, 52.13s/it]æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.7:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:52<00:52, 52.13s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][ALoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 100.99it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 101.12it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 90.69it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 101.47it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 102.31it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 90.58it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 97.95it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 98.06it/s]

GPU-0 Infer:   0%|          | 0/2 [00:00<?, ?it/s][A
GPU-0 Infer:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  2.43it/s][A
GPU-0 Infer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:22<00:00, 13.22s/it][A
                                                          [A
âœ… ç»“æœå·²ä¿å­˜è‡³: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal/temp-0.7.jsonl
æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:43<00:00, 51.60s/it]æ¨¡å‹: Qwen_Qwen2.5-7B-Instruct, Temp: 0.7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:43<00:00, 51.68s/it]

ğŸ‰ æ‰€æœ‰æ¨ç†ä»»åŠ¡å®Œæˆï¼
âœ… [æ­¥éª¤ 1/3] æ‰¹é‡æ¨ç†å®Œæˆã€‚
--------------------------------------------------
ğŸ§¹ [æ­¥éª¤ 2/3] è¿‡æ»¤ç»“æœ...
è¾“å‡ºç›®å½• '/data/nnotaa/align-anything/projects/Bio/benchmark/results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered' å·²å‡†å¤‡å°±ç»ªã€‚

æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal/temp-0.0.jsonl
å¤„ç†å®Œæˆ: 'temp-0.0.jsonl'ã€‚
  æ€»å…±å¤„ç† 13 è¡Œï¼Œä¿ç•™äº† 0 è¡Œã€‚

æ­£åœ¨å¤„ç†æ–‡ä»¶: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal/temp-0.7.jsonl
å¤„ç†å®Œæˆ: 'temp-0.7.jsonl'ã€‚
  æ€»å…±å¤„ç† 13 è¡Œï¼Œä¿ç•™äº† 1 è¡Œã€‚

æ‰€æœ‰ .jsonl æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼
âœ… [æ­¥éª¤ 2/3] è¿‡æ»¤å®Œæˆã€‚ç»“æœä½äº 'filtered_results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal'.
--------------------------------------------------
ğŸ”¬ [æ­¥éª¤ 3/3] ä½¿ç”¨ ESM æ¨¡å‹å¤„ç†åºåˆ—...
W1022 21:48:12.311000 155820 site-packages/torch/distributed/run.py:774] 
W1022 21:48:12.311000 155820 site-packages/torch/distributed/run.py:774] *****************************************
W1022 21:48:12.311000 155820 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 21:48:12.311000 155820 site-packages/torch/distributed/run.py:774] *****************************************
é«˜æ€§èƒ½æ¨¡å¼å¯åŠ¨ã€‚å…± 8 ä¸ªGPUã€‚
æœ€å¤§Tokens/æ‰¹æ¬¡: 8192
è‡ªåŠ¨æ··åˆç²¾åº¦(autocast)å·²ã€ç¦ç”¨ã€‘ã€‚
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.49it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.80it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.58it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.68it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.59it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.38it/s]
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank6]:[W1022 21:48:23.859358386 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank7]:[W1022 21:48:23.432606495 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.41it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 24.01it/s]
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank5]:[W1022 21:48:24.404702099 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank4]:[W1022 21:48:24.418820050 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1022 21:48:25.722254669 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1022 21:48:25.783586275 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1022 21:48:26.122434768 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
Rank 0 æ¨¡å‹åŠ è½½å®Œæˆã€‚
/data/nnotaa/miniconda3/envs/align-anything/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1022 21:48:26.197130090 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
æ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s]æ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s, å¤„ç†ä¸­: temp-0.0.jsonl]
Batches (Rank 0): 0batch [00:00, ?batch/s][A
                                          [Aæ–‡ä»¶æ€»è¿›åº¦:   0%|          | 0/2 [00:00<?, ?file/s, å¤„ç†ä¸­: temp-0.7.jsonl]
Batches (Rank 0): 0batch [00:00, ?batch/s][A
Batches (Rank 0): 1batch [00:00,  1.15batch/s][A
                                              [Aæ–‡ä»¶æ€»è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.29file/s, å¤„ç†ä¸­: temp-0.7.jsonl]æ–‡ä»¶æ€»è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.29file/s, å¤„ç†ä¸­: temp-0.7.jsonl]

æ‰€æœ‰è¿›ç¨‹è®¡ç®—å®Œæˆï¼Œç­‰å¾…åŒæ­¥...
å¼€å§‹åˆå¹¶ä¸´æ—¶æ–‡ä»¶...
åˆå¹¶æ–‡ä»¶:   0%|          | 0/2 [00:00<?, ?file/s]åˆå¹¶æ–‡ä»¶: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 2317.94file/s]
æ­£åœ¨ç”Ÿæˆæœ€ç»ˆçš„CSVæ‘˜è¦æ–‡ä»¶...
ç”Ÿæˆæ‘˜è¦:   0%|          | 0/2 [00:00<?, ?file/s]ç”Ÿæˆæ‘˜è¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 20763.88file/s]
æ‘˜è¦æ–‡ä»¶æˆåŠŸä¿å­˜åˆ°: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/summary_results.csv
âœ… [æ­¥éª¤ 3/3] åºåˆ—å¤„ç†å®Œæˆã€‚
--------------------------------------------------
ğŸ”¬ [Step 4/3] cleaning <|im_end|>...
*** è­¦å‘Šï¼šè„šæœ¬å°†ç›´æ¥è¦†ç›–åŸå§‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿å·²å¤‡ä»½ï¼ ***

å¼€å§‹æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm
ç›®æ ‡å­—æ®µ: assistant
ç§»é™¤æ ‡è®°: <|im_end|>

--- æ­£åœ¨å¤„ç† (å°†è¦†ç›–): results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.0.jsonl ---
å¤„ç†å®Œæˆ: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.0.jsonl (å·²è¦†ç›–)
  æ€»è¡Œæ•°: 0
  ä¿®æ”¹è¡Œæ•°: 0

--- æ­£åœ¨å¤„ç† (å°†è¦†ç›–): results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.7.jsonl ---
å¤„ç†å®Œæˆ: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.7.jsonl (å·²è¦†ç›–)
  æ€»è¡Œæ•°: 1
  ä¿®æ”¹è¡Œæ•°: 1

æ‰¹é‡å¤„ç†å®Œæ¯•ï¼Œå…±å¤„ç† 2 ä¸ªæ–‡ä»¶ã€‚
âœ… [Step 4/3] <|im_end|> cleaning completed.
--------------------------------------------------
å¼€å§‹è½¬æ¢æ–‡ä»¶: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.0.jsonl -> results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.0.fasta

è½¬æ¢å®Œæˆï¼
æˆåŠŸå†™å…¥ 0 ä¸ª FASTA æ¡ç›®ã€‚
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
Traceback (most recent call last):
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/bin/toxinpred2", line 7, in <module>
    sys.exit(main())
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/lib/python3.9/site-packages/toxinpred2/python_scripts/toxinpred2.py", line 328, in main
    aac_comp(seq,'seq.aac')
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/lib/python3.9/site-packages/toxinpred2/python_scripts/toxinpred2.py", line 58, in aac_comp
    zz = df.iloc[:,0]
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/lib/python3.9/site-packages/pandas/core/indexing.py", line 1185, in __getitem__
    return self._getitem_tuple(key)
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/lib/python3.9/site-packages/pandas/core/indexing.py", line 1691, in _getitem_tuple
    tup = self._validate_tuple_indexer(tup)
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/lib/python3.9/site-packages/pandas/core/indexing.py", line 967, in _validate_tuple_indexer
    self._validate_key(k, i)
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/lib/python3.9/site-packages/pandas/core/indexing.py", line 1593, in _validate_key
    self._validate_integer(key, axis)
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/lib/python3.9/site-packages/pandas/core/indexing.py", line 1686, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds

ERROR conda.cli.main_run:execute(127): `conda run toxinpred2 -i results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.0.fasta -o results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.0_toxinpred_results.csv -t 0.6 -m 1 -d 2` failed. (See above for error)
##############################################################################
# The program ToxinPred2.0 is developed for predicting Toxin and non toxin #
# protein from their primary sequence, developed by Prof G. P. S. Raghava's group. #
# ############################################################################
Summary of Parameters:
Input File:  results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.0.fasta ; Model:  1 ; Threshold:  0.6
Output File:  results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.0_toxinpred_results.csv ; Display:  2

å¼€å§‹è½¬æ¢æ–‡ä»¶: results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.7.jsonl -> results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.7.fasta

è½¬æ¢å®Œæˆï¼
æˆåŠŸå†™å…¥ 1 ä¸ª FASTA æ¡ç›®ã€‚
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
<frozen importlib._bootstrap>:488: Warning: OpenSSL 3's legacy provider failed to load. Legacy algorithms will not be available. If you need those algorithms, check your OpenSSL configuration.
Traceback (most recent call last):
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/bin/toxinpred2", line 7, in <module>
    sys.exit(main())
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/lib/python3.9/site-packages/toxinpred2/python_scripts/toxinpred2.py", line 330, in main
    prediction('seq.aac',nf_path+'/../model/RF_model.onnx','seq.pred' )
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/lib/python3.9/site-packages/toxinpred2/python_scripts/toxinpred2.py", line 82, in prediction
    y_p_score1=sess.run([label_name], {input_name: X_test.astype(numpy.float32)})[0]
  File "/data/nnotaa/miniconda3/envs/toxinpred2_env/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 220, in run
    return self._sess.run(output_names, input_feed, run_options)
onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: float_input Got: 1 Expected: 2 Please fix either the inputs/outputs or the model.

ERROR conda.cli.main_run:execute(127): `conda run toxinpred2 -i results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.7.fasta -o results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.7_toxinpred_results.csv -t 0.6 -m 1 -d 2` failed. (See above for error)
##############################################################################
# The program ToxinPred2.0 is developed for predicting Toxin and non toxin #
# protein from their primary sequence, developed by Prof G. P. S. Raghava's group. #
# ############################################################################
Summary of Parameters:
Input File:  results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.7.fasta ; Model:  1 ; Threshold:  0.6
Output File:  results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.7_toxinpred_results.csv ; Display:  2

å¼€å§‹å¤„ç†...
  - ä» CSV 'results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.0_toxinpred_results.csv' æå–å­—æ®µ: ['ML_Score', 'Prediction']
  - ä» JSONL 'dataset/test.jsonl' æå–å­—æ®µ: ['id', 'description', 'sequence', 'taxonomy', 'prompt']
é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ° - [Errno 2] No such file or directory: 'results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.0_toxinpred_results.csv'
å¼€å§‹å¤„ç†...
  - ä» CSV 'results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.7_toxinpred_results.csv' æå–å­—æ®µ: ['ML_Score', 'Prediction']
  - ä» JSONL 'dataset/test.jsonl' æå–å­—æ®µ: ['id', 'description', 'sequence', 'taxonomy', 'prompt']
é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ° - [Errno 2] No such file or directory: 'results/2025-10-22_21-34-41/Qwen_Qwen2.5-7B-Instruct/Animal_filtered_esm/temp-0.7_toxinpred_results.csv'
âœ… æ¨¡å‹ Qwen/Qwen2.5-7B-Instruct å¤„ç†å®Œæˆã€‚
-----------------------------------------
ğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼
