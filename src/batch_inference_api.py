#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€ API æ‰¹é‡æ¨ç†è„šæœ¬ (V3.3)

åŠŸèƒ½:
- æ¥å£ä¸ `batch_inference_local.py` å…¼å®¹ (ç§»é™¤äº† --provider)ã€‚
- å‡è®¾æ‰€æœ‰ API å‡ä¸ OpenAI æ ¼å¼å…¼å®¹ã€‚
- ä» `.env` æ–‡ä»¶è‡ªåŠ¨åŠ è½½ç»Ÿä¸€çš„ NEWAPI_API_KEY å’Œ API_BASE_URLã€‚
- [V2.5 (å¹¶å‘ç‰ˆ)] ä½¿ç”¨ ThreadPoolExecutor å®ç°å¹¶å‘è¯·æ±‚ã€‚
- [V2.5 (å¹¶å‘ç‰ˆ)] å°†å…³é”®å‚æ•°ç§»è‡³é¡¶éƒ¨çš„ CONFIG ç±»ï¼Œæ–¹ä¾¿ä¿®æ”¹ã€‚
- [V2.5 (å¹¶å‘ç‰ˆ)] ä»…å¯¹ 429 é€Ÿç‡é™åˆ¶é”™è¯¯è¿›è¡ŒæŒ‡æ•°é€€é¿ã€‚
- [V2.7] ä¸ºæ‰€æœ‰ API å®¢æˆ·ç«¯æ·»åŠ ç»Ÿä¸€çš„è¯·æ±‚è¶…æ—¶ (REQUEST_TIMEOUT_SECONDS)ã€‚
- [V3.1] ç§»é™¤ .strip() ä»¥ä¿ç•™åŸå§‹è¾“å‡ºã€‚
- [V3.2] åœ¨è¾“å‡ºçš„ JSONL ä¸­é¢å¤–æ·»åŠ  'finish_reason' å­—æ®µã€‚
- [V3.3] è¾“å‡ºçš„ JSONL ä¿ç•™è¾“å…¥ JSONL çš„æ‰€æœ‰å­—æ®µï¼Œå¹¶å°† 'assistant' 
          å’Œ 'finish_reason' ä½œä¸ºæ–°å­—æ®µå¹¶åˆ—æ·»åŠ ã€‚

ä¾èµ–åº“ (è¯·ç¡®ä¿å·²å®‰è£…):
    pip install "openai>=1.0" "python-dotenv" "tqdm"

ä½œè€…: ç”± Gemini ä¸º align-anything æ¡†æ¶ç”Ÿæˆ (V3.3 ç»Ÿä¸€ç‰ˆ)
"""

import os
import json
import time
import argparse
import random # ç”¨äºæŒ‡æ•°é€€é¿çš„æŠ–åŠ¨
import concurrent.futures # ç”¨äºå¹¶å‘å¤„ç†
import functools # ç”¨äºåå‡½æ•°
from tqdm import tqdm
from metrics import write_metric, metric_timer
from dotenv import load_dotenv
from typing import Dict, Any, List, Optional, Tuple

# å¯¼å…¥å„ä¾›åº”å•† SDK
from openai import (
    OpenAI,
    APIError as OpenAIAPIError,
    RateLimitError as OpenAIRateLimitError,
    APITimeoutError as OpenAIAPITimeoutError,
)

# ==============================================================================
# --- è„šæœ¬æ ¸å¿ƒé…ç½® ---
# ==============================================================================
class CONFIG:
    """
    åœ¨æ­¤å¤„ç»Ÿä¸€é…ç½®è„šæœ¬çš„å…³é”®å‚æ•°
    """
    
    # --- å¹¶å‘ã€é‡è¯•ä¸è¶…æ—¶é…ç½® ---
    # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    MAX_CONCURRENT_REQUESTS = 10 
    
    # API è¯·æ±‚å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•° (ä»…é™ 429 é”™è¯¯)
    MAX_RETRIES = 5 

    # API è¶…æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°
    MAX_TIMEOUT_RETRIES = 3

    # ä»»åŠ¡åœ¨é‡åˆ°â€œç¡¬è¶…æ—¶(ç¡¬æ€§æ€»æœŸé™)â€æˆ–è¿”å›è¶…æ—¶å ä½åï¼Œæœ€é«˜å…è®¸çš„é‡æ–°æŠ•é€’æ¬¡æ•°
    # æ³¨ï¼šè¿™æ˜¯åœ¨ process_single_prompt å†…éƒ¨è¶…æ—¶é‡è¯•ä¹‹å¤–çš„â€œä»»åŠ¡çº§â€é‡è¯•æ¬¡æ•°
    MAX_RESPAWN_ON_TIMEOUT = 2

    # æŒ‡æ•°é€€é¿çš„åŸºç¡€ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    BASE_WAIT_TIME_SECONDS = 1

    # API è¯·æ±‚çš„å…¨å±€è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    REQUEST_TIMEOUT_SECONDS = 1500
    # æ¯ä¸ªæ ·æœ¬çš„ç¡¬æ€§æ€»æœŸé™ï¼ˆç§’ï¼‰ï¼šç”¨äºå…œåº•é˜²å¡æ­»ï¼Œè¶…è¿‡å³æ ‡è®°ä¸ºè¶…æ—¶å†™å ä½å¹¶è·³è¿‡ç­‰å¾…
    HARD_DEADLINE_MARGIN_SECONDS = 2000
    # å°†ä»»åŠ¡æ ‡è®°ä¸ºâ€œæ…¢ä»»åŠ¡â€çš„é˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œç”¨äºå®æ—¶å¯è§†åŒ–å°¾éƒ¨å»¶è¿Ÿ
    SLOW_TASK_SLA_SECONDS = 20

    # --- Prompt å†…å®¹é…ç½® ---

    # ç³»ç»Ÿæç¤ºè¯­
    SYSTEM_PROMPT = "You are a helpful assistant."

    # æ·»åŠ åˆ°ç”¨æˆ· prompt æœ«å°¾çš„æŒ‡ä»¤åç¼€
    # USER_PROMPT_SUFFIX = ''
    USER_PROMPT_SUFFIX = '\nAttention: This is all the information. Please provide the most likely protein sequence. The answer should only contain a single sequence of single-letter codes for the 20 amino acids that make up the protein. Do not use three-letter codes. Do not include any.'
# ==============================================================================
# --- é…ç½®ç»“æŸ ---
# ==============================================================================


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚
    """
    parser = argparse.ArgumentParser(
        description="ç»Ÿä¸€ API æ‰¹é‡æ¨ç†è„šæœ¬ (V3.3)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # --- æ–‡ä»¶ I/O é…ç½® (ä¸ local è„šæœ¬ä¸€è‡´) ---
    parser.add_argument(
        "--prompt_file_path", type=str, required=True,
        help="è¾“å…¥çš„ prompt æ–‡ä»¶è·¯å¾„ (JSONL æ ¼å¼)ã€‚"
    )
    parser.add_argument(
        "--prompt_key", type=str, default="generated_prompt_for_sequence_model",
        help="JSONL æ–‡ä»¶ä¸­åŒ…å« prompt æ–‡æœ¬çš„é”® (key)ã€‚"
    )
    parser.add_argument(
        "--output_dir", type=str, required=True,
        help="è¾“å‡ºç»“æœçš„æ ¹ç›®å½•ã€‚"
    )

    # --- æ¨¡å‹ä¸æ¨ç†é…ç½® (ä¸ local è„šæœ¬ä¸€è‡´) ---
    parser.add_argument(
        "--model_paths", type=str, nargs='+', required=True,
        help="éœ€è¦è¿›è¡Œæ¨ç†çš„ä¸€ä¸ªæ¨¡å‹åç§° (ä¾‹å¦‚ 'gpt-5' æˆ– 'claude-sonnet-4-5-20250929')ã€‚"
    )
    parser.add_argument(
        "--temperatures", type=float, nargs='+', required=True,
        help="éœ€è¦æµ‹è¯•çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ¸©åº¦å€¼ã€‚"
    )

    # --- ç”Ÿæˆå‚æ•°é…ç½® (ä¸ local è„šæœ¬ä¸€è‡´) ---
    parser.add_argument(
        "--max_new_tokens", type=int, default=1024,
        help="ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ã€‚"
    )
    parser.add_argument(
        "--top_p", type=float, default=0.8,
        help="Top-p (nucleus) é‡‡æ ·å‚æ•°ã€‚"
    )
    parser.add_argument(
        "--repetition_penalty", type=float, default=1.0,
        help="é‡å¤æƒ©ç½šç³»æ•° (æ˜ å°„åˆ° frequency_penalty)ã€‚"
    )
    parser.add_argument(
        "--metric_file", type=str, default=None,
        help="ç»“æ„åŒ–æŒ‡æ ‡è¾“å‡ºæ–‡ä»¶ (JSONL)ï¼Œå¯é€‰ã€‚"
    )
    # --- å†å²ç»“æœå¤ç”¨ï¼ˆå¯é€‰ï¼‰ ---
    parser.add_argument(
        "--reuse_dir", type=str, default=None,
        help="å¯é€‰ï¼šæŒ‡å®šä¸€ä¸ªå†å²è¾“å‡ºç›®å½•ï¼Œè‹¥è¯¥ç›®å½•ä¸‹å­˜åœ¨ temp-{T}.jsonlï¼Œåˆ™å¯¹å·²å­˜åœ¨çš„æ ·æœ¬ç›´æ¥å¤ç”¨ç»“æœï¼›è‹¥è¯¥æ ·æœ¬çš„ finish_reason ä¸º 'api_error' åˆ™ä¸å¤ç”¨è€Œé‡æ–°æ¨ç†ã€‚"
    )
    parser.add_argument(
        "--reuse_key", type=str, default="id",
        help="ç”¨äºåŒ¹é…æ ·æœ¬çš„é”®ï¼Œé»˜è®¤ä½¿ç”¨ 'id'ï¼›è‹¥ç¼ºå¤±åˆ™é€€åŒ–ä¸ºä½¿ç”¨ prompt_key å¯¹åº”çš„æ–‡æœ¬ä½œä¸ºåŒ¹é…é”®ã€‚"
    )
    parser.add_argument(
        "--reuse_dirs", type=str, nargs='+', default=None,
        help="å¯é€‰ï¼šæä¾›å¤šä¸ªå†å²è¾“å‡ºç›®å½•ï¼Œå°†é€’å½’æ‰«ææ‰€æœ‰ç›®å½•ä¸‹çš„ temp-{T}.jsonl å¹¶åˆå¹¶å¤ç”¨æ˜ å°„ï¼ˆä¼˜å…ˆäº --reuse_dirï¼‰ã€‚"
    )
    
    return parser.parse_args()

def load_prompt_data(file_path, prompt_key):
    """
    [V3.3] ä» JSONL æ–‡ä»¶åŠ è½½æ‰€æœ‰ promptsï¼Œä¿ç•™åŸå§‹å­—å…¸ç»“æ„ã€‚
    """
    data_list = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    record = json.loads(line)
                    if prompt_key not in record:
                        print(f"è­¦å‘Š: è·³è¿‡ç¼ºå°‘é”® '{prompt_key}' çš„è¡Œ: {line.strip()}")
                        continue
                    data_list.append(record)
                except (json.JSONDecodeError) as e:
                    print(f"è­¦å‘Š: è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ: {line.strip()} - é”™è¯¯: {e}")
    except FileNotFoundError:
        print(f"é”™è¯¯: Prompt æ–‡ä»¶æœªæ‰¾åˆ° -> {file_path}")
        return None
    return data_list

# --- [V3.0] ç»Ÿä¸€çš„ API å®¢æˆ·ç«¯ ---
class ApiClient:
    """
    æ­¤ç±»ä½¿ç”¨ OpenAI SDKï¼Œç”¨äºæ‰€æœ‰å…¼å®¹ OpenAI æ ¼å¼çš„ API è°ƒç”¨ã€‚
    """
    def __init__(self, model_name, args, api_key, base_url):
        self.model_name = model_name
        self.args = args # å­˜å‚¨æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°ä»¥å¤‡åç”¨
        if args.repetition_penalty != 1.0:
            print(f"ä¿¡æ¯: 'repetition_penalty' (å€¼: {self.args.repetition_penalty}) å°†è¢«æ˜ å°„åˆ° OpenAI çš„ 'frequency_penalty'ã€‚")

        # è‡ªåŠ¨ä¿®æ­£ .env æ–‡ä»¶ä¸­å¤šä½™çš„ /chat/completions åç¼€
        suffix_to_remove = "/chat/completions"
        if base_url.endswith(suffix_to_remove):
            original_url = base_url
            base_url = base_url[:-len(suffix_to_remove)]
            print(f"è­¦å‘Š: æ£€æµ‹åˆ° base_url åŒ…å« '{suffix_to_remove}'ã€‚")
            print(f"     å·²è‡ªåŠ¨å°†å…¶ä¿®æ­£ä¸º: {base_url} (åŸå§‹: {original_url})")

        self.client = OpenAI(
            api_key=api_key, 
            base_url=base_url,
            timeout=CONFIG.REQUEST_TIMEOUT_SECONDS 
        )
        print(f"ç»Ÿä¸€ API å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {model_name}ï¼ŒURL: {base_url}")


    def generate(self, prompt, temperature):
        messages = [
            {"role": "system", "content": CONFIG.SYSTEM_PROMPT},
            {"role": "user", "content": prompt + CONFIG.USER_PROMPT_SUFFIX}
        ]
        # æ˜ å°„ repetition_penalty -> frequency_penalty
        freq_penalty = 0.0 if self.args.repetition_penalty == 1.0 else (self.args.repetition_penalty - 1.0)
        
        # [V2.2 ä¿®å¤] ç§»é™¤äº† top_p å‚æ•°
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            temperature=temperature if temperature > 0 else 0.0,
            max_tokens=self.args.max_new_tokens,
            frequency_penalty=freq_penalty
        )
        
        # [V3.2] è·å–å†…å®¹å’Œç»“æŸåŸå› 
        content = response.choices[0].message.content
        finish_reason = response.choices[0].finish_reason
        
        # [V3.2] è¿”å›å†…å®¹å’Œç»“æŸåŸå›  (V3.1 ç§»é™¤äº† .strip())
        return content, finish_reason

    def is_rate_limit_error(self, error):
        """[V2.5 å›é€€] æ£€æŸ¥ OpenAI SDK æŠ›å‡ºçš„æ˜¯å¦ä¸º 429 é”™è¯¯"""
        if isinstance(error, OpenAIRateLimitError):
            return True
        # æŸäº›å¼‚å¸¸ç±»å‹å¯èƒ½æ²¡æœ‰ status_code å±æ€§
        if isinstance(error, OpenAIAPIError):
            code = getattr(error, 'status_code', None)
            return code == 429
        return False


# --- [V2.5 æ–°å¢] å•ä¸ª prompt çš„å¤„ç†å‡½æ•° (ç”¨äºå¹¶å‘) ---
def process_single_prompt(input_data, client, temp, prompt_key):
    """
    [V3.3] å¤„ç†å•ä¸ª prompt æ•°æ® (å­—å…¸)ï¼ŒåŒ…å«å®Œæ•´çš„æŒ‡æ•°é€€é¿é‡è¯•é€»è¾‘ã€‚
    æ­¤å‡½æ•°è¢«è®¾è®¡ä¸ºåœ¨å¹¶å‘çº¿ç¨‹ä¸­è¿è¡Œã€‚
    
    [V3.3] è¿”å›: ä¸€ä¸ªå­—å…¸, åŒ…å« input_data çš„æ‰€æœ‰å­—æ®µï¼Œå¹¶æ·»åŠ  "assistant" å’Œ "finish_reason"ã€‚
    """
    
    # [V3.3] å¤åˆ¶åŸå§‹æ•°æ®ä»¥ä¿ç•™æ‰€æœ‰å­—æ®µ
    output_record = input_data.copy()
    generated_text = ""
    finish_reason = "unknown" 

    try:
        prompt_text = input_data[prompt_key]
    except KeyError:
        print(f"\né”™è¯¯: åœ¨ process_single_prompt ä¸­æœªæ‰¾åˆ° prompt_key '{prompt_key}'ã€‚æ•°æ®: {input_data}")
        output_record['assistant'] = "ERROR: Missing prompt_key in input data."
        output_record['finish_reason'] = "preprocessing_error"
        return output_record
    
    rate_attempts = 0
    timeout_attempts = 0
    while True:
        try:
            # 1. å°è¯•è°ƒç”¨ API
            generated_text, finish_reason = client.generate(
                prompt=prompt_text,
                temperature=temp
            )
            # æˆåŠŸ
            break

        except OpenAIAPITimeoutError as e:
            # 2. è¶…æ—¶é‡è¯•ï¼ˆä¸ 429 åˆ†å¼€è®¡æ•°ï¼‰
            if timeout_attempts < CONFIG.MAX_TIMEOUT_RETRIES:
                wait_time = (CONFIG.BASE_WAIT_TIME_SECONDS * (2 ** timeout_attempts)) + random.uniform(0, 3)
                timeout_attempts += 1
                print(f"\nè­¦å‘Š: API è¶…æ—¶ï¼Œå°†åœ¨ {wait_time:.1f}s åé‡è¯• (timeout {timeout_attempts}/{CONFIG.MAX_TIMEOUT_RETRIES}).")
                time.sleep(wait_time)
                continue
            else:
                print(f"\né”™è¯¯: API è¶…æ—¶é‡è¯•å·²è¾¾ä¸Šé™ ({CONFIG.MAX_TIMEOUT_RETRIES})ï¼Œæ”¾å¼ƒè¯¥æ ·æœ¬ã€‚")
                generated_text = f"API_TIMEOUT: {e}"
                finish_reason = "timeout"
                break

        except OpenAIRateLimitError as e:
            # 3. é€Ÿç‡é™åˆ¶ï¼ˆ429ï¼‰é‡è¯•
            if rate_attempts < CONFIG.MAX_RETRIES:
                wait_time = (CONFIG.BASE_WAIT_TIME_SECONDS * (2 ** rate_attempts)) + random.uniform(0, 3)
                rate_attempts += 1
                print(f"\nè­¦å‘Š: 429 é€Ÿç‡é™åˆ¶ï¼Œå°†åœ¨ {wait_time:.1f}s åé‡è¯• (rate {rate_attempts}/{CONFIG.MAX_RETRIES}).")
                time.sleep(wait_time)
                continue
            else:
                print(f"\né”™è¯¯: 429 é‡è¯•å·²è¾¾ä¸Šé™ ({CONFIG.MAX_RETRIES})ï¼Œæ”¾å¼ƒè¯¥æ ·æœ¬ã€‚")
                generated_text = f"API_ERROR: Max retries exceeded for Rate Limit. {e}"
                finish_reason = "max_retries_exceeded"
                break

        except OpenAIAPIError as e:
            # 4. å…¶ä»– API é”™è¯¯ï¼ˆ400/500ç­‰ï¼‰ï¼Œä¸é‡è¯•
            print(f"\né”™è¯¯: API è°ƒç”¨å¤±è´¥ (status_code={getattr(e,'status_code',None)}): {e}")
            generated_text = f"API_ERROR: {e}"
            finish_reason = "api_error"
            break

        except Exception as e:
            # 5. å…¶ä»–æœªçŸ¥é”™è¯¯ï¼Œä¸é‡è¯•
            print(f"\né”™è¯¯: å‘ç”ŸæœªçŸ¥é”™è¯¯ (Prompt: '{prompt_text[:50]}...'): {e}")
            generated_text = f"UNKNOWN_ERROR: {e}"
            finish_reason = "unknown_error"
            break

    # 7. [V3.3] å°†ç»“æœåˆå¹¶å›åŸå§‹å­—å…¸
    output_record['assistant'] = generated_text
    output_record['finish_reason'] = finish_reason
    return output_record


def build_sample_key(record: Dict[str, Any], prompt_key: str, reuse_key: str) -> str:
    """ç”Ÿæˆç”¨äºåŒ¹é…çš„æ ·æœ¬é”®ï¼šä¼˜å…ˆä½¿ç”¨ reuse_keyï¼›è‹¥ä¸å­˜åœ¨åˆ™ç”¨ prompt æ–‡æœ¬ã€‚"""
    if reuse_key and reuse_key in record:
        return str(record.get(reuse_key))
    return str(record.get(prompt_key, ""))


def _iter_temp_files(reuse_dirs: List[str], temp: float) -> List[str]:
    target_name = f"temp-{temp}.jsonl"
    files: List[str] = []
    for d in reuse_dirs:
        if not d:
            continue
        if os.path.isfile(d) and os.path.basename(d) == target_name:
            files.append(d)
            continue
        if not os.path.isdir(d):
            continue
        for root, _, fls in os.walk(d):
            for fn in fls:
                if fn == target_name:
                    files.append(os.path.join(root, fn))
    return files


def _infer_model_and_domain_from_path(path: str) -> Tuple[Optional[str], Optional[str]]:
    """ä»æ–‡ä»¶è·¯å¾„æ¨æ–­ (model, domain)ã€‚
    è§„åˆ™ï¼šè·¯å¾„ç‰‡æ®µä¸­è‹¥åŒ…å« 'Animal' æˆ– 'Bacteria'ï¼Œåˆ™å°†å…¶ä½œä¸º domainï¼Œä¸”å…¶å‰ä¸€ä¸ªç‰‡æ®µä½œä¸º modelã€‚
    ä¾‹ï¼š.../results/<run>/<model>/Bacteria/raw/temp-0.7.jsonl â†’ (model, 'Bacteria')
    """
    parts = os.path.normpath(path).split(os.sep)
    domain = None
    model = None
    for i, p in enumerate(parts):
        if p in ("Animal", "Bacteria"):
            domain = p
            if i > 0:
                model = parts[i - 1]
            break
    return model, domain


def _infer_domain_from_output_dir(output_dir: str) -> Optional[str]:
    parts = os.path.normpath(output_dir).split(os.sep)
    if "Animal" in parts:
        return "Animal"
    if "Bacteria" in parts:
        return "Bacteria"
    return None


def load_reuse_map(
    reuse_dirs: List[str],
    temp: float,
    prompt_key: str,
    reuse_key: str,
    current_model: str,
    current_domain: Optional[str],
) -> Dict[str, Dict[str, Any]]:
    """ä»ä¸€ä¸ªæˆ–å¤šä¸ªå¤ç”¨ç›®å½•ï¼ˆé€’å½’æ‰«æï¼‰åŠ è½½ temp-{temp}.jsonlï¼Œæ„å»º key -> {assistant, finish_reason} çš„æ˜ å°„ã€‚
    ä»…ä¿ç•™ä¸å½“å‰ (model, domain) ä¸€è‡´çš„æ¡ç›®ï¼Œé¿å…è·¨æ¨¡å‹/è·¨ç‰©ç§å¤ç”¨ã€‚
    """
    mapping: Dict[str, Dict[str, Any]] = {}
    files = _iter_temp_files(reuse_dirs, temp)
    for path in files:
        try:
            with open(path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        rec = json.loads(line)
                    except Exception:
                        continue
                    # æ¨æ–­è¯¥è®°å½•çš„ model/domain
                    rec_model = rec.get('model')
                    rec_domain = rec.get('domain')
                    if not rec_model or not rec_domain:
                        pm, pd = _infer_model_and_domain_from_path(path)
                        rec_model = rec_model or pm
                        rec_domain = rec_domain or pd

                    # è¿‡æ»¤ï¼šå¿…é¡»ä¸å½“å‰ (model, domain) åŒ¹é…ï¼ˆè‹¥ current_domain æ— æ³•æ¨æ–­ï¼Œåˆ™ä»…æ¯”å¯¹ modelï¼‰
                    if rec_model != current_model:
                        continue
                    if current_domain is not None and rec_domain != current_domain:
                        continue

                    key = build_sample_key(rec, prompt_key, reuse_key)
                    if not key:
                        continue
                    mapping[key] = {
                        'assistant': rec.get('assistant', ''),
                        'finish_reason': rec.get('finish_reason', ''),
                    }
        except Exception:
            continue
    return mapping


def is_error_like_output(finish_reason: str, assistant_text: str) -> bool:
    """åˆ¤æ–­å†å²ç»“æœæ˜¯å¦å±äºé”™è¯¯/ä¸å¯å¤ç”¨ï¼šæ‰€æœ‰é”™è¯¯éƒ½å¼ºåˆ¶é‡æ¨ï¼›åªè¦æ˜¯æ¨¡å‹çœŸå®è¾“å‡ºå°±å¯å¤ç”¨ã€‚
    è§„åˆ™ï¼š
    - finish_reason åœ¨ä»¥ä¸‹é›†åˆåˆ™è§†ä¸ºé”™è¯¯ï¼š{api_error, timeout, timeout_hard, max_retries_exceeded, worker_error, unknown_error}
    - æˆ– assistant æ–‡æœ¬ä»¥å·²çŸ¥é”™è¯¯å‰ç¼€å¼€å¤´ï¼šAPI_TIMEOUT, API_ERROR, UNKNOWN_ERROR, WORKER_ERROR
    - å…¶ä»–ï¼ˆå¦‚ stop/length/content_filter/æ­£å¸¸æ–‡æœ¬ï¼‰è§†ä¸ºå¯å¤ç”¨ã€‚
    """
    fr = (finish_reason or "").lower()
    if fr in {"api_error", "timeout", "timeout_hard", "max_retries_exceeded", "worker_error", "unknown_error"}:
        return True
    at = assistant_text or ""
    err_prefixes = ("API_TIMEOUT", "API_ERROR", "UNKNOWN_ERROR", "WORKER_ERROR")
    if any(at.startswith(p) for p in err_prefixes):
        return True
    return False

# --- ä¸»æ‰§è¡Œå‡½æ•° (V3.3 æ›´æ–°) ---
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    args = parse_args()
    
    dotenv_path = os.path.join(os.path.dirname(__file__), '.env')
    load_dotenv(dotenv_path=dotenv_path)
    
    model_name = args.model_paths[0]
    
    print("="*60)
    print(f"ğŸš€ å¼€å§‹ ç»Ÿä¸€API æ¨ç†ä»»åŠ¡ (å¹¶å‘ç‰ˆ v3.3)...") # [V3.3]
    print(f"âš¡ æœ€å¤§å¹¶å‘æ•°: {CONFIG.MAX_CONCURRENT_REQUESTS}")
    print(f" timeout={CONFIG.REQUEST_TIMEOUT_SECONDS}s")
    print(f"ğŸ¤– API æ¨¡å‹: {model_name}")
    print(f"ğŸ“‚ Prompt æ–‡ä»¶: {args.prompt_file_path}")
    print(f"ğŸ”¥ æ¸©åº¦åˆ—è¡¨: {args.temperatures}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print("="*60)
    os.makedirs(args.output_dir, exist_ok=True)
    
    # [V3.3] åŠ è½½å®Œæ•´çš„ prompt æ•°æ®ï¼Œè€Œä¸ä»…ä»…æ˜¯æ–‡æœ¬
    all_prompt_data = load_prompt_data(args.prompt_file_path, args.prompt_key)
    if not all_prompt_data:
        print("âŒ Prompt æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    try:
        # [V3.0] åˆå§‹åŒ–ç»Ÿä¸€å®¢æˆ·ç«¯
        api_key = os.getenv("NEWAPI_API_KEY")
        if not api_key:
            raise ValueError("é”™è¯¯: æœªæ‰¾åˆ° NEWAPI_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")
            
        base_url = os.getenv("API_BASE_URL") # [V3.0] ä½¿ç”¨æ–°çš„ç»Ÿä¸€ URL
        if not base_url:
            raise ValueError("é”™è¯¯: API_BASE_URL æœªåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ã€‚")

        client = ApiClient(model_name, args, api_key, base_url)

    except ValueError as e:
        print(e)
        return

    for temp in args.temperatures:
        print(f"\nğŸ”„ -----------------------------------")
        print(f"ğŸ”„ æ­£åœ¨å¤„ç†æ¸©åº¦: {temp}...")
        print(f"ğŸ”„ -----------------------------------")
        
        output_filename = f"temp-{temp}.jsonl"
        output_path = os.path.join(args.output_dir, output_filename)
        
        num_written = 0
        num_reused = 0
        num_reinfer_due_to_error = 0
        with open(output_path, 'w', encoding='utf-8') as f_out:
            # [V3.4] ä½¿ç”¨â€œæœ‰ç•Œå¹¶å‘ + ç»“æœå…œåº•â€çš„æ–¹å¼ï¼Œé¿å…å•æ ·æœ¬å¡æ­»å¯¼è‡´è¿›åº¦æ¡åœåœ¨æœ€åä¸€ä¸ª
            worker_func = functools.partial(
                process_single_prompt,
                client=client,
                temp=temp,
                prompt_key=args.prompt_key
            )

            hard_deadline_per_task = CONFIG.REQUEST_TIMEOUT_SECONDS * (CONFIG.MAX_TIMEOUT_RETRIES + 1) + CONFIG.HARD_DEADLINE_MARGIN_SECONDS

            # è®°å½•æ¯ä¸ªæ ·æœ¬çš„â€œä»»åŠ¡çº§é‡è¯•â€å°è¯•æ¬¡æ•°ä¸æ˜¯å¦æ›¾å‘ç”Ÿè¶…æ—¶
            attempts_by_idx = {}
            had_timeout_before = {}
            respawned_timeouts = 0
            recovered_timeouts = 0

            with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG.MAX_CONCURRENT_REQUESTS) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡ï¼Œè®°å½•èµ·å§‹æ—¶é—´ä¸ç´¢å¼•ï¼Œä¾¿äºå…œåº•è¾“å‡º
                futures = {}
                start_times = {}
                # å†å²å¤ç”¨æ˜ å°„ï¼ˆæ”¯æŒå¤šä¸ªç›®å½•ï¼›è‹¥ä»…æä¾› --reuse_dir ä¹Ÿå…¼å®¹ï¼‰
                reuse_dirs: List[str] = []
                if args.reuse_dirs:
                    reuse_dirs.extend(args.reuse_dirs)
                elif args.reuse_dir:
                    reuse_dirs.append(args.reuse_dir)
                # å½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡
                current_domain = _infer_domain_from_output_dir(args.output_dir)
                reuse_map = load_reuse_map(
                    reuse_dirs, temp, args.prompt_key, args.reuse_key, model_name, current_domain
                ) if reuse_dirs else {}
                pbar = tqdm(total=len(all_prompt_data), desc=f"Temp-{temp} Infer")
                forced_timeouts = 0
                for idx, item in enumerate(all_prompt_data):
                    # åˆ¤æ–­æ˜¯å¦å¯å¤ç”¨
                    reused_here = False
                    if reuse_map:
                        key = build_sample_key(item, args.prompt_key, args.reuse_key)
                        if key in reuse_map:
                            prev = reuse_map[key]
                            if is_error_like_output(prev.get('finish_reason', ''), prev.get('assistant', '')):
                                # æ ‡è®°éœ€é‡æ¨
                                num_reinfer_due_to_error += 1
                            else:
                                # ç›´æ¥å¤ç”¨ï¼šä»¥å½“å‰è¾“å…¥å­—æ®µä¸ºåŸºï¼Œè¦†ç›– assistant/finish_reason
                                out = item.copy()
                                out['assistant'] = prev.get('assistant', '')
                                out['finish_reason'] = prev.get('finish_reason', '')
                                f_out.write(json.dumps(out, ensure_ascii=False) + '\n')
                                num_written += 1
                                num_reused += 1
                                pbar.update(1)
                                reused_here = True
                    if reused_here:
                        continue

                    fut = executor.submit(worker_func, item)
                    futures[fut] = idx
                    start_times[fut] = time.time()
                    attempts_by_idx[idx] = attempts_by_idx.get(idx, 0) + 1
                    had_timeout_before[idx] = False

                # å¾ªç¯ç­‰å¾…å®Œæˆæˆ–è¶…æ—¶çš„ä»»åŠ¡ï¼›å®šæœŸæ£€æŸ¥å¡ä½çš„ future å¹¶å…œåº•
                while futures:
                    done, not_done = concurrent.futures.wait(
                        futures.keys(), timeout=2, return_when=concurrent.futures.FIRST_COMPLETED
                    )

                    # å¤„ç†å·²å®Œæˆçš„ä»»åŠ¡
                    for fut in done:
                        idx = futures.pop(fut)
                        start_times.pop(fut, None)
                        try:
                            output_record = fut.result()
                        except Exception as e:
                            # ä¸åº”å¸¸è§ï¼›å…œåº•å†™å…¥é”™è¯¯å ä½
                            base = all_prompt_data[idx].copy()
                            base['assistant'] = f"WORKER_ERROR: {e}"
                            base['finish_reason'] = 'worker_error'
                            output_record = base

                        # å¦‚æœç»“æœæ˜¯â€œè¶…æ—¶â€ç±»ï¼Œå ä½æˆ–è½¯è¶…æ—¶ï¼Œå°è¯•ä»»åŠ¡çº§é‡æŠ•é€’
                        is_timeout_like = False
                        try:
                            fr = str(output_record.get('finish_reason', '')).lower()
                            assistant_txt = str(output_record.get('assistant', ''))
                            if fr in ('timeout', 'timeout_hard') or assistant_txt.startswith('API_TIMEOUT'):
                                is_timeout_like = True
                        except Exception:
                            pass

                        if is_timeout_like and attempts_by_idx.get(idx, 0) < CONFIG.MAX_RESPAWN_ON_TIMEOUT + 1:
                            # è®°å½•æ›¾å‘ç”Ÿè¶…æ—¶
                            had_timeout_before[idx] = True
                            # ä»»åŠ¡çº§é‡æŠ•é€’
                            respawned_timeouts += 1
                            fut2 = executor.submit(worker_func, all_prompt_data[idx])
                            futures[fut2] = idx
                            start_times[fut2] = time.time()
                            attempts_by_idx[idx] = attempts_by_idx.get(idx, 0) + 1
                            # ä¸å†™å…¥å ä½ï¼Œä¹Ÿä¸æ›´æ–°è¿›åº¦æ¡ï¼Œç­‰å¾…æ–°çš„ç»“æœ
                        else:
                            # æœ€ç»ˆè½ç›˜
                            if had_timeout_before.get(idx, False) and not is_timeout_like:
                                recovered_timeouts += 1
                            f_out.write(json.dumps(output_record, ensure_ascii=False) + '\n')
                            num_written += 1
                            # åŠ¨æ€åç¼€æ›´æ–°ï¼ˆå®Œæˆä¸€ä¸ªåä¹Ÿåˆ·æ–°ä¸€æ¬¡ï¼‰
                            now2 = time.time()
                            inflight = len(not_done)
                            slow = sum(1 for f in not_done if (now2 - start_times.get(f, now2)) > CONFIG.SLOW_TASK_SLA_SECONDS)
                            oldest_age = 0 if inflight == 0 else max((now2 - start_times.get(f, now2)) for f in not_done)
                            pbar.update(1)
                            pbar.set_postfix({
                                'inflight': inflight,
                                'slow': slow,
                                'oldest_s': f"{int(oldest_age)}",
                                'forced_to': forced_timeouts,
                            })

                    # æ£€æŸ¥â€œç¡¬æ€§æ€»æœŸé™â€æ˜¯å¦è¶…æ—¶çš„æœªå®Œæˆä»»åŠ¡ï¼Œåšå…œåº•è¾“å‡ºå¹¶ä¸å†ç­‰å¾…
                    now = time.time()
                    expired = [f for f in not_done if (now - start_times.get(f, now)) > hard_deadline_per_task]
                    for f in expired:
                        idx = futures.pop(f)
                        start_times.pop(f, None)
                        # å¦‚æœå°šå¯è¿›è¡Œä»»åŠ¡çº§é‡æŠ•é€’ï¼Œåˆ™é‡æŠ•è€Œä¸æ˜¯ç«‹åˆ»å†™å…¥å ä½
                        if attempts_by_idx.get(idx, 0) < CONFIG.MAX_RESPAWN_ON_TIMEOUT + 1:
                            had_timeout_before[idx] = True
                            respawned_timeouts += 1
                            fut2 = executor.submit(worker_func, all_prompt_data[idx])
                            futures[fut2] = idx
                            start_times[fut2] = time.time()
                            attempts_by_idx[idx] = attempts_by_idx.get(idx, 0) + 1
                            # ä¸æ›´æ–° pbarï¼Œç­‰å¾…æ–°çš„ç»“æœ
                        else:
                            # è¾¾åˆ°é‡æŠ•ä¸Šé™ï¼Œè½ç›˜å ä½å¹¶è®¡ä¸ºå¼ºåˆ¶è¶…æ—¶
                            base = all_prompt_data[idx].copy()
                            base['assistant'] = "API_TIMEOUT: hard-deadline exceeded"
                            base['finish_reason'] = 'timeout_hard'
                            f_out.write(json.dumps(base, ensure_ascii=False) + '\n')
                            num_written += 1
                            forced_timeouts += 1
                            pbar.update(1)
                            # åˆ·æ–°åç¼€
                            inflight2 = len(not_done) - 1 if len(not_done) > 0 else 0
                            slow2 = sum(1 for nf in not_done if (now - start_times.get(nf, now)) > CONFIG.SLOW_TASK_SLA_SECONDS)
                            oldest_age2 = 0 if inflight2 == 0 else max((now - start_times.get(nf, now)) for nf in not_done)
                            pbar.set_postfix({
                                'inflight': inflight2,
                                'slow': slow2,
                                'oldest_s': f"{int(oldest_age2)}",
                                'forced_to': forced_timeouts,
                            })

                pbar.close()
        
        print(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {output_path}")
        # è®°å½•æ¯ä¸ªæ¸©åº¦çš„ä¸€æ¡æŒ‡æ ‡
        write_metric(
            args.metric_file,
            step="batch_inference_api",
            data={
                "model": model_name,
                "temperature": temp,
                "output_path": output_path,
                "num_prompts": len(all_prompt_data),
                "num_outputs": num_written,
                "forced_timeouts": locals().get('forced_timeouts', 0),
                "respawned_timeouts": locals().get('respawned_timeouts', 0),
                "recovered_timeouts": locals().get('recovered_timeouts', 0),
            }
        )
        
    print("\nğŸ‰ æ‰€æœ‰ API æ¨ç†ä»»åŠ¡å®Œæˆï¼")
    # è®°å½•ä»»åŠ¡çº§æ‘˜è¦
    write_metric(
        args.metric_file,
        step="batch_inference_api_summary",
        data={
            "model": model_name,
            "temperatures": args.temperatures,
            "output_dir": args.output_dir,
        }
    )

if __name__ == "__main__":
    main()