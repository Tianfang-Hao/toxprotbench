#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šä¾›åº”å•† API æ‰¹é‡æ¨ç†è„šæœ¬ (æ¥å£ç»Ÿä¸€ç‰ˆ)

åŠŸèƒ½:
- æ¥å£ä¸ `batch_inference_local.py` å…¼å®¹ã€‚
- æ–°å¢ `--provider` å‚æ•°ï¼Œç”¨äºé€‰æ‹© API ä¾›åº”å•†ã€‚
- æ”¯æŒ: 'openai', 'anthropic', 'deepseek', 'google' (é€šè¿‡è‡ªå®šä¹‰ Base URL)ã€‚
- ä» `.env` æ–‡ä»¶è‡ªåŠ¨åŠ è½½ç»Ÿä¸€çš„ NEWAPI_API_KEY å’Œ å„è‡ªçš„ Base URLã€‚
- [V2.5 (å¹¶å‘ç‰ˆ)] ä½¿ç”¨ ThreadPoolExecutor å®ç°å¹¶å‘è¯·æ±‚ã€‚
- [V2.5 (å¹¶å‘ç‰ˆ)] å°†å…³é”®å‚æ•°ç§»è‡³é¡¶éƒ¨çš„ CONFIG ç±»ï¼Œæ–¹ä¾¿ä¿®æ”¹ã€‚
- [V2.5 (å¹¶å‘ç‰ˆ)] å›é€€ V2.4 çš„ä¿®æ”¹ï¼Œä»…å¯¹ 429 é€Ÿç‡é™åˆ¶é”™è¯¯è¿›è¡ŒæŒ‡æ•°é€€é¿ã€‚
- [V2.6] å°† 'gemini' é‡å‘½åä¸º 'google'ã€‚
- ä¸ºæ¯ä¸ªæ¸©åº¦ç”Ÿæˆä¸€è‡´çš„ JSONL æ ¼å¼è¾“å‡ºã€‚
- å¯æ‰©å±•è®¾è®¡ï¼šä½¿ç”¨æŠ½è±¡åŸºç±»å’Œå·¥å‚æ¨¡å¼ï¼Œæ–¹ä¾¿æœªæ¥æ·»åŠ æ–°çš„ APIã€‚

ä¾èµ–åº“ (è¯·ç¡®ä¿å·²å®‰è£…):
    pip install "openai>=1.0" "anthropic>=0.20" "python-dotenv" "tqdm" "requests"

ä½œè€…: ç”± Gemini ä¸º align-anything æ¡†æ¶ç”Ÿæˆ (V2.6 å¹¶å‘ç‰ˆ)
"""

import os
import json
import time
import argparse
import abc  # å¯¼å…¥æŠ½è±¡åŸºç±»æ¨¡å—
import random # ç”¨äºæŒ‡æ•°é€€é¿çš„æŠ–åŠ¨
import concurrent.futures # ç”¨äºå¹¶å‘å¤„ç†
import functools # ç”¨äºåå‡½æ•°
from tqdm import tqdm
from metrics import write_metric
from dotenv import load_dotenv
import requests # ç”¨äº Google REST API

# å¯¼å…¥å„ä¾›åº”å•† SDK
from openai import OpenAI, APIError as OpenAIAPIError, RateLimitError as OpenAIRateLimitError
import importlib
try:
    anthropic = importlib.import_module('anthropic')
except Exception:
    anthropic = None

# ==============================================================================
# --- è„šæœ¬æ ¸å¿ƒé…ç½® ---
# ==============================================================================
class CONFIG:
    """
    åœ¨æ­¤å¤„ç»Ÿä¸€é…ç½®è„šæœ¬çš„å…³é”®å‚æ•°
    """
    
    # --- å¹¶å‘ä¸é‡è¯•é…ç½® ---
    
    # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    # è°ƒé«˜æ­¤å€¼å¯åŠ å¿«å¤„ç†é€Ÿåº¦ï¼Œä½†è¯·æ³¨æ„ä¸è¦è¶…è¿‡æ‚¨ API ä»£ç†çš„é€Ÿç‡é™åˆ¶
    MAX_CONCURRENT_REQUESTS = 10 
    
    # API è¯·æ±‚å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•° (ä»…é™ 429 é”™è¯¯)
    MAX_RETRIES = 5

    # æŒ‡æ•°é€€é¿çš„åŸºç¡€ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    # ç¬¬ä¸€æ¬¡é‡è¯•ç­‰å¾… (10s * 2^0) +æŠ–åŠ¨, ç¬¬äºŒæ¬¡ (10s * 2^1) +æŠ–åŠ¨, ...
    BASE_WAIT_TIME_SECONDS = 1

    # --- Prompt å†…å®¹é…ç½® ---

    # ç³»ç»Ÿæç¤ºè¯­
    SYSTEM_PROMPT = "You are a helpful assistant."

    # æ·»åŠ åˆ°ç”¨æˆ· prompt æœ«å°¾çš„æŒ‡ä»¤åç¼€
    USER_PROMPT_SUFFIX = '\nAttention: This is all the information. Please provide the most likely protein sequence. The answer should only contain the single-letter codes for the 20 amino acids that make up the protein. Do not use three-letter codes or include any other characters, including spaces.'
    USER_PROMPT_SUFFIX = ''
# ==============================================================================
# --- é…ç½®ç»“æŸ ---
# ==============================================================================


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚
    æ¥å£ä¸ `batch_inference_local.py` ä¿æŒä¸€è‡´, æ–°å¢ --providerã€‚
    """
    parser = argparse.ArgumentParser(
        description="å¤šä¾›åº”å•† API æ‰¹é‡æ¨ç†è„šæœ¬ (æ¥å£ç»Ÿä¸€ç‰ˆ)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    # --- æ–°å¢å‚æ•° ---
    parser.add_argument(
        "--provider", type=str, required=True,
        choices=['openai', 'anthropic', 'deepseek', 'google'], # [V2.6] gemini -> google
        help="é€‰æ‹©è¦è°ƒç”¨çš„ API ä¾›åº”å•†ã€‚"
    )
    
    # --- æ–‡ä»¶ I/O é…ç½® (ä¸ local è„šæœ¬ä¸€è‡´) ---
    parser.add_argument(
        "--prompt_file_path", type=str, required=True,
        help="è¾“å…¥çš„ prompt æ–‡ä»¶è·¯å¾„ (JSONL æ ¼å¼)ã€‚"
    )
    parser.add_argument(
        "--prompt_key", type=str, default="generated_prompt_for_sequence_model",
        help="JSONL æ–‡ä»¶ä¸­åŒ…å« prompt æ–‡æœ¬çš„é”® (key)ã€‚"
    )
    parser.add_argument(
        "--output_dir", type=str, required=True,
        help="è¾“å‡ºç»“æœçš„æ ¹ç›®å½•ã€‚"
    )

    # --- æ¨¡å‹ä¸æ¨ç†é…ç½® (ä¸ local è„šæœ¬ä¸€è‡´) ---
    parser.add_argument(
        "--model_paths", type=str, nargs='+', required=True,
        help="éœ€è¦è¿›è¡Œæ¨ç†çš„ä¸€ä¸ªæ¨¡å‹åç§° (ä¾‹å¦‚ 'gpt-5' æˆ– 'claude-sonnet-4-5-20250929')ã€‚"
    )
    parser.add_argument(
        "--temperatures", type=float, nargs='+', required=True,
        help="éœ€è¦æµ‹è¯•çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ¸©åº¦å€¼ã€‚"
    )

    # --- ç”Ÿæˆå‚æ•°é…ç½® (ä¸ local è„šæœ¬ä¸€è‡´) ---
    parser.add_argument(
        "--max_new_tokens", type=int, default=1024,
        help="ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ã€‚"
    )
    parser.add_argument(
        "--top_p", type=float, default=0.8,
        help="Top-p (nucleus) é‡‡æ ·å‚æ•°ã€‚"
    )
    parser.add_argument(
        "--repetition_penalty", type=float, default=1.0,
        help="é‡å¤æƒ©ç½šç³»æ•° (ä»…éƒ¨åˆ† API æ”¯æŒ)ã€‚"
    )
    parser.add_argument(
        "--metric_file", type=str, default=None,
        help="ç»“æ„åŒ–æŒ‡æ ‡è¾“å‡ºæ–‡ä»¶ (JSONL)ï¼Œå¯é€‰ã€‚"
    )
    
    return parser.parse_args()

def load_prompts(file_path, prompt_key):
    """ä» JSONL æ–‡ä»¶åŠ è½½æ‰€æœ‰ prompts (ä¸ local è„šæœ¬ä¸€è‡´)"""
    prompts = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    prompts.append(json.loads(line)[prompt_key])
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"è­¦å‘Š: è·³è¿‡æ ¼å¼é”™è¯¯æˆ–ç¼ºå°‘é”® '{prompt_key}' çš„è¡Œ: {line.strip()} - é”™è¯¯: {e}")
    except FileNotFoundError:
        print(f"é”™è¯¯: Prompt æ–‡ä»¶æœªæ‰¾åˆ° -> {file_path}")
        return None
    return prompts

# --- API å®¢æˆ·ç«¯æŠ½è±¡åŸºç±» ---
class AbstractChatClient(abc.ABC):
    """
    API å®¢æˆ·ç«¯çš„æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰äº†ç»Ÿä¸€çš„æ¥å£ã€‚
    """
    def __init__(self, model_name, args):
        self.model_name = model_name
        self.args = args # å­˜å‚¨æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°ä»¥å¤‡åç”¨
        if args.repetition_penalty != 1.0:
            self.warn_repetition_penalty()

    def warn_repetition_penalty(self):
        # é»˜è®¤è­¦å‘Šï¼Œå­ç±»å¯ä»¥è¦†ç›–
        print(f"è­¦å‘Š: {self.__class__.__name__} å¯èƒ½ä¸å®Œå…¨æ”¯æŒ repetition_penaltyï¼Œè¯¥å‚æ•°å°†è¢«è¿‘ä¼¼æˆ–å¿½ç•¥ã€‚")

    @abc.abstractmethod
    def generate(self, prompt, temperature):
        """
        æ‰€æœ‰å­ç±»å¿…é¡»å®ç°çš„æ¨ç†æ–¹æ³•ã€‚
        å¿…é¡»è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸² (ç”Ÿæˆçš„æ–‡æœ¬)ã€‚
        """
        pass
    
    def is_rate_limit_error(self, error):
        """
        [V2.5 å›é€€] æ£€æŸ¥ä¼ å…¥çš„å¼‚å¸¸æ˜¯å¦ä¸º 429 é€Ÿç‡é™åˆ¶é”™è¯¯ã€‚
        """
        return False

# --- OpenAI å’Œ DeepSeek (å…¼å®¹) å®¢æˆ·ç«¯å®ç° ---
class OpenAICompatibleClient(AbstractChatClient):
    """
    æ­¤ç±»åŒæ—¶é€‚ç”¨äº OpenAI å’Œ DeepSeekï¼Œå› ä¸ºå®ƒä»¬å…±äº«ç›¸åŒçš„ API æ ¼å¼ã€‚
    """
    def __init__(self, model_name, args, api_key, base_url):
        super().__init__(model_name, args)
        
        # è‡ªåŠ¨ä¿®æ­£ .env æ–‡ä»¶ä¸­å¤šä½™çš„ /chat/completions åç¼€
        suffix_to_remove = "/chat/completions"
        if base_url.endswith(suffix_to_remove):
            original_url = base_url
            base_url = base_url[:-len(suffix_to_remove)]
            print(f"è­¦å‘Š: æ£€æµ‹åˆ° base_url åŒ…å« '{suffix_to_remove}'ã€‚")
            print(f"     å·²è‡ªåŠ¨å°†å…¶ä¿®æ­£ä¸º: {base_url} (åŸå§‹: {original_url})")

        self.client = OpenAI(api_key=api_key, base_url=base_url)
        print(f"OpenAI å…¼å®¹å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {model_name}ï¼ŒURL: {base_url}")

    def warn_repetition_penalty(self):
        print(f"ä¿¡æ¯: 'repetition_penalty' (å€¼: {self.args.repetition_penalty}) å°†è¢«æ˜ å°„åˆ° OpenAI çš„ 'frequency_penalty'ã€‚")

    def generate(self, prompt, temperature):
        messages = [
            {"role": "system", "content": CONFIG.SYSTEM_PROMPT},
            {"role": "user", "content": prompt + CONFIG.USER_PROMPT_SUFFIX}
        ]
        # æ˜ å°„ repetition_penalty -> frequency_penalty
        freq_penalty = 0.0 if self.args.repetition_penalty == 1.0 else (self.args.repetition_penalty - 1.0)
        
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            temperature=temperature if temperature > 0 else 0.0,
            max_tokens=self.args.max_new_tokens,
            # [V2.2 ä¿®å¤] ç§»é™¤äº† top_p å‚æ•°
            frequency_penalty=freq_penalty
        )
        return response.choices[0].message.content.strip()

    def is_rate_limit_error(self, error):
        """[V2.5 å›é€€] æ£€æŸ¥ OpenAI SDK æŠ›å‡ºçš„æ˜¯å¦ä¸º 429 é”™è¯¯"""
        return isinstance(error, OpenAIRateLimitError) or \
               (isinstance(error, OpenAIAPIError) and error.status_code == 429)


# --- Anthropic (Claude) å®¢æˆ·ç«¯å®ç° ---
class AnthropicClient(AbstractChatClient):
    def __init__(self, model_name, args, api_key, base_url):
        super().__init__(model_name, args)
        
        suffix_to_remove = "/messages"
        if base_url.endswith(suffix_to_remove):
            original_url = base_url
            base_url = base_url[:-len(suffix_to_remove)]
            print(f"è­¦å‘Š: æ£€æµ‹åˆ° base_url åŒ…å« '{suffix_to_remove}'ã€‚")
            print(f"     å·²è‡ªåŠ¨å°†å…¶ä¿®æ­£ä¸º: {base_url} (åŸå§‹: {original_url})")
        
        self.client = anthropic.Anthropic(api_key=api_key, base_url=base_url)
        print(f"Anthropic å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {model_name}ï¼ŒURL: {base_url}")

    def warn_repetition_penalty(self):
        print("è­¦å‘Š: Anthropic (Claude) API ä¸æ”¯æŒ repetition_penaltyï¼Œè¯¥å‚æ•°å°†è¢«å¿½ç•¥ã€‚")

    def generate(self, prompt, temperature):
        response = self.client.messages.create(
            model=self.model_name,
            system=CONFIG.SYSTEM_PROMPT,
            messages=[
                {"role": "user", "content": prompt + CONFIG.USER_PROMPT_SUFFIX}
            ],
            temperature=temperature,
            max_tokens=self.args.max_new_tokens,
            top_p=self.args.top_p
        )
        return response.content[0].text.strip()

    def is_rate_limit_error(self, error):
        """[V2.5 å›é€€] æ£€æŸ¥ Anthropic SDK æŠ›å‡ºçš„æ˜¯å¦ä¸º 429 é”™è¯¯"""
        return isinstance(error, anthropic.RateLimitError) or \
               (isinstance(error, anthropic.APIError) and error.status_code == 429)


# --- [V2.6] Google (REST API) å®¢æˆ·ç«¯å®ç° ---
class GoogleRestClient(AbstractChatClient):
    """
    æ­¤ç±»ä½¿ç”¨ 'requests' åº“æ¥ç²¾ç¡®æ¨¡æ‹Ÿæ‚¨çš„ curl ç¤ºä¾‹ã€‚
    [V2.6] é‡å‘½åè‡ª GeminiRestClient
    """
    def __init__(self, model_name, args, api_key, base_url):
        super().__init__(model_name, args)
        self.api_key = api_key
        self.url = f"{base_url}/models/{model_name}:generateContent?key={self.api_key}"
        print(f"Google (REST) å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {model_name}ï¼ŒURL: {base_url}/models/{model_name}:...")

    def warn_repetition_penalty(self):
        print("è­¦å‘Š: Google (Gemini) API ä¸æ”¯æŒ repetition_penaltyï¼Œè¯¥å‚æ•°å°†è¢«å¿½ç•¥ã€‚")

    def generate(self, prompt, temperature):
        payload = {
            "contents": [
                {"role": "user", "parts": [{"text": prompt + CONFIG.USER_PROMPT_SUFFIX}]}
            ],
            "systemInstruction": {
                "parts": [{"text": CONFIG.SYSTEM_PROMPT}]
            },
            "generationConfig": {
                "temperature": temperature,
                "topP": self.args.top_p,
                "maxOutputTokens": self.args.max_new_tokens,
                "candidateCount": 1
            }
        }
        headers = {'Content-Type': 'application/json'}
        
        response = requests.post(self.url, headers=headers, json=payload)
        response.raise_for_status() 
        data = response.json()
        
        if "candidates" not in data or not data["candidates"]:
            print(f"è­¦å‘Š: Google API è¿”å›äº†ç©º 'candidates'ã€‚å“åº”: {data}")
            return "GOOGLE_ERROR: No candidates returned."
            
        return data['candidates'][0]['content']['parts'][0]['text'].strip()

    def is_rate_limit_error(self, error):
        """[V2.5 å›é€€] æ£€æŸ¥ requests æŠ›å‡ºçš„æ˜¯å¦ä¸º 429 é”™è¯¯"""
        return isinstance(error, requests.HTTPError) and error.response.status_code == 429

# --- å®¢æˆ·ç«¯å·¥å‚å‡½æ•° ---
def get_client(provider, model_name, args):
    """
    æ ¹æ® provider åç§°å’Œæ¨¡å‹åç§°ï¼Œåˆå§‹åŒ–å¹¶è¿”å›å¯¹åº”çš„ API å®¢æˆ·ç«¯ã€‚
    """
    api_key = os.getenv("NEWAPI_API_KEY")
    if not api_key:
        raise ValueError("é”™è¯¯: æœªæ‰¾åˆ° NEWAPI_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")

    if provider == "openai":
        base_url = os.getenv("OPENAI_BASE_URL")
        if not base_url:
            raise ValueError("é”™è¯¯: OPENAI_BASE_URL æœªåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ã€‚")
        return OpenAICompatibleClient(model_name, args, api_key, base_url)
        
    elif provider == "deepseek":
        base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")
        if not base_url:
            raise ValueError("é”™è¯¯: DEEPSEEK_BASE_URL æœªåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ã€‚")
        return OpenAICompatibleClient(model_name, args, api_key, base_url)

    elif provider == "anthropic":
        base_url = os.getenv("ANTHROPIC_BASE_URL")
        if not base_url:
            raise ValueError("é”™è¯¯: ANTHROPIC_BASE_URL æœªåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ã€‚")
        return AnthropicClient(model_name, args, api_key, base_url)
        
    elif provider == "google": # [V2.6] gemini -> google
        base_url = os.getenv("GOOGLE_BASE_URL") # [V2.6] GEMINI_BASE_URL -> GOOGLE_BASE_URL
        if not base_url:
            raise ValueError("é”™è¯¯: GOOGLE_BASE_URL æœªåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ã€‚")
        return GoogleRestClient(model_name, args, api_key, base_url) # [V2.6] GeminiRestClient -> GoogleRestClient
        
    else:
        raise ValueError(f"é”™è¯¯: ä¸æ”¯æŒçš„ provider: {provider}")

# --- [V2.5 æ–°å¢] å•ä¸ª prompt çš„å¤„ç†å‡½æ•° (ç”¨äºå¹¶å‘) ---
def process_single_prompt(prompt_text, client, temp):
    """
    å¤„ç†å•ä¸ª promptï¼ŒåŒ…å«å®Œæ•´çš„æŒ‡æ•°é€€é¿é‡è¯•é€»è¾‘ã€‚
    æ­¤å‡½æ•°è¢«è®¾è®¡ä¸ºåœ¨å¹¶å‘çº¿ç¨‹ä¸­è¿è¡Œã€‚
    
    è¿”å›: ä¸€ä¸ªå­—å…¸, {"prompt": "...", "assistant": "..."}
    """
    generated_text = ""
    
    for attempt in range(CONFIG.MAX_RETRIES):
        try:
            # 1. å°è¯•è°ƒç”¨ API
            generated_text = client.generate(
                prompt=prompt_text,
                temperature=temp
            )
            # 2. å¦‚æœæˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
            break 
            
        except (OpenAIAPIError, anthropic.APIError, requests.RequestException) as e:
            
            # 3. æ£€æŸ¥æ˜¯å¦ä¸º 429 é€Ÿç‡é™åˆ¶é”™è¯¯
            if client.is_rate_limit_error(e):
                if attempt < CONFIG.MAX_RETRIES - 1:
                    # è®¡ç®—é€€é¿æ—¶é—´ï¼š(2^n * åŸºç¡€æ—¶é—´) + éšæœºæŠ–åŠ¨
                    wait_time = (CONFIG.BASE_WAIT_TIME_SECONDS * (2 ** attempt)) + random.uniform(0, 3)
                    print(f"\nè­¦å‘Š: æ”¶åˆ° 429 é€Ÿç‡é™åˆ¶é”™è¯¯ (Prompt: '{prompt_text[:20]}...'). "
                          f"å°†åœ¨ {wait_time:.1f} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{CONFIG.MAX_RETRIES} æ¬¡)...")
                    time.sleep(wait_time)
                else:
                    # 4. è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                    print(f"\né”™è¯¯: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({CONFIG.MAX_RETRIES})ã€‚æ”¾å¼ƒæ­¤ promptã€‚é”™è¯¯: {e}")
                    generated_text = f"API_ERROR: Max retries exceeded for Rate Limit. {e}"
                    break # æ”¾å¼ƒå¹¶è·³å‡ºå¾ªç¯
            else:
                # 5. å¦‚æœæ˜¯å…¶ä»– API é”™è¯¯ (å¦‚ 400, 500, 503)
                print(f"\né”™è¯¯: API è°ƒç”¨å¤±è´¥ (ä¸å¯é‡è¯•é”™è¯¯): {e}")
                generated_text = f"API_ERROR: {e}"
                break # ä¸é‡è¯•é 429 é”™è¯¯
                
        except Exception as e:
            # 6. æ•è·å…¶ä»–æœªçŸ¥ Python é”™è¯¯
            print(f"\né”™è¯¯: å‘ç”ŸæœªçŸ¥é”™è¯¯ (Prompt: '{prompt_text[:50]}...'): {e}")
            generated_text = f"UNKNOWN_ERROR: {e}"
            break # ä¸é‡è¯•æœªçŸ¥é”™è¯¯

    # 7. è¿”å›ç»“æœ
    return {
        "prompt": prompt_text,
        "assistant": generated_text 
    }

# --- ä¸»æ‰§è¡Œå‡½æ•° (V2.5 æ›´æ–°) ---
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    args = parse_args()
    
    dotenv_path = os.path.join(os.path.dirname(__file__), '.env')
    load_dotenv(dotenv_path=dotenv_path)
    
    model_name = args.model_paths[0]
    
    print("="*60)
    print(f"ğŸš€ å¼€å§‹ API æ¨ç†ä»»åŠ¡ (å¹¶å‘ç‰ˆ v2.6)...") # [V2.6]
    print(f"âš¡ æœ€å¤§å¹¶å‘æ•°: {CONFIG.MAX_CONCURRENT_REQUESTS}")
    print(f"ğŸ¢ ä¾›åº”å•†: {args.provider}")
    print(f"ğŸ¤– API æ¨¡å‹: {model_name}")
    print(f"ğŸ“‚ Prompt æ–‡ä»¶: {args.prompt_file_path}")
    print(f"ğŸ”¥ æ¸©åº¦åˆ—è¡¨: {args.temperatures}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print("="*60)
    os.makedirs(args.output_dir, exist_ok=True)
    
    all_prompts = load_prompts(args.prompt_file_path, args.prompt_key)
    if not all_prompts:
        print("âŒ Prompt æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    try:
        # åˆå§‹åŒ–ä¸€ä¸ªå®¢æˆ·ç«¯
        client = get_client(args.provider, model_name, args)
    except ValueError as e:
        print(e)
        return

    for temp in args.temperatures:
        print(f"\nğŸ”„ æ­£åœ¨å¤„ç†æ¸©åº¦: {temp}...")
        
        output_filename = f"temp-{temp}.jsonl"
        output_path = os.path.join(args.output_dir, output_filename)
        
        # [V2.5 æ›´æ–°] ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶å‘å¤„ç†
        num_written = 0
        with open(output_path, 'w', encoding='utf-8') as f_out:
            # åˆ›å»ºä¸€ä¸ªåå‡½æ•°ï¼Œå›ºå®š client å’Œ temp å‚æ•°
            worker_func = functools.partial(process_single_prompt, client=client, temp=temp)
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG.MAX_CONCURRENT_REQUESTS) as executor:
                # executor.map ä¼šä¿æŒä¸ all_prompts ç›¸åŒçš„é¡ºåºè¿”å›ç»“æœ
                results_iterator = executor.map(worker_func, all_prompts)
                
                # ä½¿ç”¨ tqdm åŒ…è£…è¿­ä»£å™¨ä»¥æ˜¾ç¤ºè¿›åº¦
                for output_record in tqdm(results_iterator, total=len(all_prompts), desc=f"Temp-{temp} Infer"):
                    f_out.write(json.dumps(output_record, ensure_ascii=False) + '\n')
                    num_written += 1
        
        print(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {output_path}")
        write_metric(
            args.metric_file,
            step="batch_inference_api_multi",
            data={
                "provider": args.provider,
                "model": model_name,
                "temperature": temp,
                "output_path": output_path,
                "num_prompts": len(all_prompts),
                "num_outputs": num_written,
            }
        )
        
    print("\nğŸ‰ æ‰€æœ‰ API æ¨ç†ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    main()

