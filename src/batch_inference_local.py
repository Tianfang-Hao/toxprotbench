#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen2.5 æ¨¡å‹æ‰¹é‡åˆ†å¸ƒå¼æ¨ç†è„šæœ¬ (å‘½ä»¤è¡Œç‰ˆæœ¬)

åŠŸèƒ½:
- æ”¯æŒä» JSONL æ–‡ä»¶ä¸­è¯»å–å¤šä¸ª prompt (åŠå…¶å…³è”æ•°æ®) è¿›è¡Œæ‰¹é‡æ¨ç†ã€‚
- [å·²ä¿®æ”¹] ç”Ÿæˆçš„ JSONL æ–‡ä»¶ä¼šä¿ç•™åŸå§‹ JSONL çš„æ‰€æœ‰å­—æ®µï¼Œå¹¶æ–°å¢ä¸€ä¸ª "assistant" å­—æ®µã€‚
- é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ”¯æŒå¯¹å¤šä¸ªæœ¬åœ°æ¨¡å‹å’Œå¤šä¸ªæ¸©åº¦å‚æ•°è¿›è¡Œç»„åˆæµ‹è¯•ã€‚
- ä½¿ç”¨ PyTorch Distributed Data Parallel (DDP) åœ¨å¤š GPU ç¯å¢ƒä¸‹è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼Œæ˜¾è‘—æå‡æ•ˆç‡ã€‚
- ä¸ºæ¯ä¸ªæ¨¡å‹å’Œæ¸©åº¦çš„ç»„åˆç”Ÿæˆç‹¬ç«‹çš„ JSONL ç»“æœæ–‡ä»¶ã€‚
- æä¾›è¯¦ç»†çš„è¿›åº¦æ¡æ¥ç›‘æ§æ•´ä¸ªæ¨ç†è¿‡ç¨‹ã€‚

ä½¿ç”¨æ–¹æ³•:
    1.  ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–åº“ã€‚
    2.  åœ¨ç»ˆç«¯ä¸­ä½¿ç”¨ `torchrun` å‘½ä»¤å¹¶é™„å¸¦æ‰€éœ€å‚æ•°æ¥å¯åŠ¨è„šæœ¬ã€‚

ç¤ºä¾‹å‘½ä»¤:
    # ä½¿ç”¨ 2 ä¸ª GPU å¯¹ä¸¤ä¸ªæ¨¡å‹ã€ä¸‰ä¸ªæ¸©åº¦è¿›è¡Œæ¨ç†
    torchrun --nproc_per_node=2 batch_inference_cli_updated.py \
        --prompt_file_path "/path/to/your/prompts.jsonl" \
        --prompt_key "prompt_text" \
        --output_dir "/path/to/save/results" \
        --model_paths "/path/to/Qwen2.5-7B-Instruct/slice_2690" "/path/to/Another-Model/checkpoint-1000" \
        --temperatures 0.0 0.7 1.0 \
        --max_new_tokens 256 \
        --top_p 0.8 \
        --repetition_penalty 1.0

ä¾èµ–åº“:
    - transformers, torch, accelerate, sentencepiece
    - tqdm (ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡)

ä½œè€…: ç”Ÿæˆè‡ª align-anything æ¡†æ¶ (ç”± Gemini ä¿®æ”¹)
æ—¥æœŸ: 2025-09-12 (æ›´æ–°äº 2025-09-13)
"""

# å¯¼å…¥å¿…è¦çš„åº“
import os
import json
import torch
import torch.distributed as dist
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
from metrics import write_metric
from datetime import datetime
import argparse
import threading
import time
from pathlib import Path


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="Qwen æ¨¡å‹æ‰¹é‡åˆ†å¸ƒå¼æ¨ç†è„šæœ¬ (å‘½ä»¤è¡Œç‰ˆæœ¬)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    # --- æ–‡ä»¶ I/O é…ç½® ---
    parser.add_argument(
        "--prompt_file_path",
        type=str,
        required=True,
        help="è¾“å…¥çš„ prompt æ–‡ä»¶è·¯å¾„ (JSONL æ ¼å¼)ã€‚"
    )
    parser.add_argument(
        "--prompt_key",
        type=str,
        default="generated_prompt_for_sequence_model",
        help="JSONL æ–‡ä»¶ä¸­åŒ…å« prompt æ–‡æœ¬çš„é”® (key)ã€‚"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        required=True,
        help="è¾“å‡ºç»“æœçš„æ ¹ç›®å½•ã€‚"
    )

    # --- æ¨¡å‹ä¸æ¨ç†é…ç½® ---
    parser.add_argument(
        "--model_paths",
        type=str,
        nargs='+',
        required=True,
        help="éœ€è¦è¿›è¡Œæ¨ç†çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ¨¡å‹è·¯å¾„ï¼Œç”¨ç©ºæ ¼åˆ†éš”ã€‚"
    )
    parser.add_argument(
        "--temperatures",
        type=float,
        nargs='+',
        required=True,
        help="éœ€è¦æµ‹è¯•çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ¸©åº¦å€¼ï¼Œç”¨ç©ºæ ¼åˆ†éš”ã€‚"
    )

    # --- ç”Ÿæˆå‚æ•°é…ç½® ---
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=256,
        help="ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ã€‚"
    )
    parser.add_argument(
        "--top_p",
        type=float,
        default=0.8,
        help="Top-p (nucleus) é‡‡æ ·å‚æ•°ã€‚"
    )
    parser.add_argument(
        "--repetition_penalty",
        type=float,
        default=1.0,
        help="é‡å¤æƒ©ç½šç³»æ•°ã€‚"
    )
    parser.add_argument(
        "--metric_file",
        type=str,
        default=None,
        help="ç»“æ„åŒ–æŒ‡æ ‡è¾“å‡ºæ–‡ä»¶ (JSONL)ï¼Œå¯é€‰ã€‚"
    )
    
    return parser.parse_args()


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if not dist.is_available() or not torch.cuda.is_available() or torch.cuda.device_count() <= 1:
        # å¦‚æœç¯å¢ƒä¸æ”¯æŒåˆ†å¸ƒå¼æˆ–åªæœ‰ä¸€ä¸ª GPUï¼Œåˆ™è¿”å›å•æœºé…ç½®
        return 0, 1
    dist.init_process_group("nccl")
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    torch.cuda.set_device(rank)
    return rank, world_size

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()

def load_data(file_path, prompt_key):
    """
    [å·²ä¿®æ”¹] ä» JSONL æ–‡ä»¶åŠ è½½æ‰€æœ‰æ•°æ® (å­—å…¸åˆ—è¡¨)
    """
    data_list = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    item = json.loads(line)
                    # ç¡®ä¿ prompt_key å­˜åœ¨
                    if prompt_key not in item:
                        raise KeyError(f"é”® '{prompt_key}' ä¸åœ¨è¡Œä¸­")
                    data_list.append(item)
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"è­¦å‘Š: è·³è¿‡æ ¼å¼é”™è¯¯æˆ–ç¼ºå°‘é”® '{prompt_key}' çš„è¡Œ: {line.strip()} - é”™è¯¯: {e}")
    except FileNotFoundError:
        print(f"é”™è¯¯: Prompt æ–‡ä»¶æœªæ‰¾åˆ° -> {file_path}")
        return None
    return data_list

def get_model_response(model, tokenizer, user_input, generation_config):
    """æ ¹æ®å•ä¸ªç”¨æˆ·è¾“å…¥ï¼Œç”Ÿæˆæ¨¡å‹å›å¤"""
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": user_input + '\nAttention: This is all the information. Please provide the most likely protein sequence. The answer should only contain the single-letter codes for the 20 amino acids that make up the protein. Do not use three-letter codes or include any other characters, including spaces.' }
    ]
    
    text = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    input_length = model_inputs.input_ids.shape[1]

    generation_kwargs = generation_config.copy()
    generation_kwargs['pad_token_id'] = tokenizer.eos_token_id
    
    outputs = model.generate(**model_inputs, **generation_kwargs)
    response_ids = outputs[0][input_length:]
    
    response_text = tokenizer.decode(response_ids, skip_special_tokens=False).strip()
    return response_text

def run_inference_on_rank(rank, world_size, model_path, temp, data_on_this_rank, base_gen_config, prompt_key):
    """
    [å·²ä¿®æ”¹] å•ä¸ª rank (GPU) ä¸Šçš„æ¨ç†æ‰§è¡Œå‡½æ•°
    """
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    ).to(rank)
    model.eval()

    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # é…ç½®å½“å‰æ¸©åº¦çš„ç”Ÿæˆå‚æ•°
    current_gen_config = base_gen_config.copy()
    if temp == 0.0:
        current_gen_config["do_sample"] = False
        current_gen_config.pop("temperature", None)
        current_gen_config.pop("top_p", None)
    else:
        current_gen_config["do_sample"] = True
        current_gen_config["temperature"] = temp

    # ä¸ºå½“å‰ rank ä¸Šçš„ prompts è¿›è¡Œæ¨ç†
    results = []
    progress_bar = tqdm(
        data_on_this_rank, # [ä¿®æ”¹] è¿­ä»£å­—å…¸åˆ—è¡¨
        desc=f"GPU-{rank} Infer", 
        disable=(rank != 0),
        position=1,
        leave=False
    )
    completed = 0
    # è½»é‡è¿›åº¦æ–‡ä»¶ï¼ˆè·¨ rank æ±‡æ€»ç”¨ï¼‰ï¼Œrank å†™å…¥ï¼Œrank0 æ±‡æ€»å±•ç¤º
    progress_dir = None
    progress_file = None
    try:
        # ä»…å½“ output_dir å¯ç”¨æ—¶åœ¨å¤–å±‚èµ‹å€¼ï¼Œè¿™é‡Œå»¶è¿Ÿåˆ°ä¸»å‡½æ•°ä¼ å‚ä¼šæ›´å¥½ï¼Œä½†ä¸ºæœ€å°æ”¹åŠ¨æš‚ç”¨ç¯å¢ƒå˜é‡ä¼ é€’
        progress_dir = os.environ.get("AA_LOCAL_PROGRESS_DIR", None)
        if progress_dir:
            Path(progress_dir).mkdir(parents=True, exist_ok=True)
            progress_file = os.path.join(progress_dir, f"rank_{rank}.progress")
            with open(progress_file, 'w') as pf:
                pf.write("0\n")
    except Exception:
        progress_file = None

    for item in progress_bar:
        # [ä¿®æ”¹] å¤åˆ¶åŸå§‹å­—å…¸ï¼Œä»¥ä¿ç•™æ‰€æœ‰å­—æ®µ
        result_item = item.copy()
        
        # [ä¿®æ”¹] ä»å­—å…¸ä¸­æå– prompt
        prompt = result_item.get(prompt_key)
        
        if prompt is None:
             # ç†è®ºä¸Š load_data å·²ç»æ£€æŸ¥è¿‡äº†ï¼Œä½†ä½œä¸ºå®‰å…¨æªæ–½
            print(f"è­¦å‘Š: GPU-{rank} å‘ç°ç¼ºå°‘ prompt_key '{prompt_key}' çš„é¡¹ç›®ï¼Œè·³è¿‡ã€‚")
            continue
        
        response = get_model_response(model, tokenizer, prompt, current_gen_config)
        
        # [ä¿®æ”¹] å°† assistant å“åº”æ·»åŠ åˆ°é¡¶å±‚
        result_item["assistant"] = response
        
        # [ä¿®æ”¹] æ·»åŠ å®Œæ•´çš„ã€ä¿®æ”¹åçš„å­—å…¸
        results.append(result_item)
        completed += 1
        # å†™å…¥è¿›åº¦è®¡æ•°ï¼ˆå°½é‡ä½é¢‘ï¼Œä½†ä¿æŒç®€å•æ¯æ¡å†™ä¸€æ¬¡ï¼Œæ–‡ä»¶æå°ï¼‰
        if progress_file and (completed % 1 == 0):
            try:
                with open(progress_file, 'w') as pf:
                    pf.write(str(completed) + "\n")
            except Exception:
                pass
        
    # æœ«å°¾æ ‡è®°å®Œæˆ
    if progress_file:
        try:
            with open(progress_file, 'w') as pf:
                pf.write("DONE\n")
        except Exception:
            pass
    return results

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    args = parse_args()
    rank, world_size = setup_distributed()

    # ä»å‘½ä»¤è¡Œå‚æ•°æ„å»ºåŸºç¡€ç”Ÿæˆé…ç½®
    base_generation_config = {
        "max_new_tokens": args.max_new_tokens,
        "top_p": args.top_p,
        "repetition_penalty": args.repetition_penalty
    }

    if rank == 0:
        print("="*60)
        print(f"ğŸš€ å¼€å§‹åˆ†å¸ƒå¼æ¨ç†ä»»åŠ¡ï¼Œå…± {world_size} ä¸ª GPUã€‚")
        print(f"ğŸ“‚ Prompt æ–‡ä»¶: {args.prompt_file_path} (Key: {args.prompt_key})")
        print(f"ğŸ¤– æ¨¡å‹åˆ—è¡¨: {args.model_paths}")
        print(f"ğŸ”¥ æ¸©åº¦åˆ—è¡¨: {args.temperatures}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        print("="*60)
        os.makedirs(args.output_dir, exist_ok=True)
        total_runs = len(args.model_paths) * len(args.temperatures)
        main_progress = tqdm(total=total_runs, desc="æ€»è¿›åº¦", position=0)

    # 1. [ä¿®æ”¹] æ‰€æœ‰ rank éƒ½åŠ è½½å®Œæ•´æ•°æ®åˆ—è¡¨
    all_data = load_data(args.prompt_file_path, args.prompt_key)
    if not all_data:
        if rank == 0:
            print("âŒ Prompt æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–ï¼Œç¨‹åºé€€å‡ºã€‚")
        cleanup_distributed()
        return
        
    # ä¸»å¾ªç¯ï¼šéå†æ¨¡å‹å’Œæ¸©åº¦
    for model_path in args.model_paths:
        torch.cuda.empty_cache()

        # åŸºäºæ¨¡å‹è·¯å¾„æ„å»ºä¸€ä¸ªæ¸…æ™°ã€é€‚åˆåšæ–‡ä»¶åçš„ model_name
        slice_name = os.path.basename(model_path)
        parent_dir_name = os.path.basename(os.path.dirname(model_path))
        model_name = f"{parent_dir_name}_{slice_name}"

        if rank == 0:
            print(f"\nğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}...")
        
        if world_size > 1:
            dist.barrier()
            
        for temp in args.temperatures:
            if rank == 0:
                main_progress.set_description(f"æ¨¡å‹: {model_name}, Temp: {temp}")

            # 2. [ä¿®æ”¹] æ¯ä¸ª rank è·å–è‡ªå·±çš„æ•°æ®å­é›†
            data_on_this_rank = all_data[rank::world_size]

            # 3. [ä¿®æ”¹] åœ¨å½“å‰ rank ä¸Šæ‰§è¡Œæ¨ç†ï¼Œä¼ å…¥ prompt_key
            # ä¸ºè¯¥æ¸©åº¦å»ºç«‹è·¨ rank è¿›åº¦ç›‘æ§ï¼ˆä»… rank0ï¼‰
            monitor_thread = None
            stop_monitor = threading.Event()
            if rank == 0 and world_size > 1:
                progress_dir = os.path.join(args.output_dir, f".progress_local_temp_{temp}")
                os.environ["AA_LOCAL_PROGRESS_DIR"] = progress_dir
                total = len(all_data)

                def monitor_func():
                    bar = tqdm(total=total, desc="æ ·æœ¬æ€»è¿›åº¦", position=2, leave=False)
                    last = 0
                    try:
                        while not stop_monitor.is_set():
                            done = 0
                            try:
                                for r in range(world_size):
                                    pfile = os.path.join(progress_dir, f"rank_{r}.progress")
                                    if os.path.exists(pfile):
                                        with open(pfile, 'r') as pf:
                                            content = pf.read().strip()
                                            if content == "DONE":
                                                # è¯¥ rank å®Œæˆæ•°é‡å³ä¸ºè¯¥ rank åˆ†é…çš„æ ·æœ¬æ•°
                                                done += len(all_data[r::world_size])
                                            else:
                                                try:
                                                    done += int(content)
                                                except Exception:
                                                    pass
                            except Exception:
                                pass
                            inc = max(done - last, 0)
                            if inc:
                                bar.update(inc)
                                last = done
                            # ç®€å•å°¾å»¶è¿Ÿè§‚æµ‹ï¼šè®¡ç®—æ¯ä¸ª rank æœªå®Œæˆæ•°é‡
                            inflight = total - done
                            bar.set_postfix({"inflight": inflight})
                            time.sleep(1.0)
                    finally:
                        bar.close()

                monitor_thread = threading.Thread(target=monitor_func, daemon=True)
                monitor_thread.start()
            local_results = run_inference_on_rank(
                rank, 
                world_size, 
                model_path, 
                temp, 
                data_on_this_rank, 
                base_generation_config,
                args.prompt_key # [ä¿®æ”¹] ä¼ å…¥ key
            )

            # 4. æ”¶é›†æ‰€æœ‰ rank çš„ç»“æœåˆ° rank 0
            all_results_gathered = None
            if world_size > 1:
                all_results_gathered = [None] * world_size
                dist.all_gather_object(all_results_gathered, local_results)
            else:
                all_results_gathered = [local_results]
            
            # 5. Rank 0 è´Ÿè´£å†™å…¥æ–‡ä»¶
            if rank == 0:
                # [ä¿®æ”¹] é‡æ–°æ’åºï¼Œä»¥åŒ¹é…åŸå§‹æ•°æ®é¡ºåº
                final_results = [None] * len(all_data)
                for i in range(world_size):
                    final_results[i::world_size] = all_results_gathered[i]
                
                # å®šä¹‰è¾“å‡ºæ–‡ä»¶å
                output_filename = f"temp-{temp}.jsonl"
                output_path = os.path.join(args.output_dir, output_filename)
                
                num_written = 0
                with open(output_path, 'w', encoding='utf-8') as f:
                    for item in final_results:
                        if item: # ç¡®ä¿ item ä¸æ˜¯ None
                            f.write(json.dumps(item, ensure_ascii=False) + '\n')
                            num_written += 1
                
                print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: {output_path}")
                main_progress.update(1)

                # å†™å‡ºæ¯æ¸©åº¦æŒ‡æ ‡ï¼ˆä»… rank 0 è®°å½•ï¼‰
                write_metric(
                    args.metric_file,
                    step="batch_inference_local",
                    data={
                        "model": model_name,
                        "temperature": temp,
                        "output_path": output_path,
                        "num_prompts": len(all_data),
                        "num_outputs": num_written,
                    }
                )

            if world_size > 1:
                dist.barrier()

            # åœæ­¢ä¸å›æ”¶ç›‘æ§
            if rank == 0 and monitor_thread is not None:
                stop_monitor.set()
                monitor_thread.join(timeout=2)

    if rank == 0:
        main_progress.close()
        print("\nğŸ‰ æ‰€æœ‰æ¨ç†ä»»åŠ¡å®Œæˆï¼")
        write_metric(
            args.metric_file,
            step="batch_inference_local_summary",
            data={
                "output_dir": args.output_dir,
                "temperatures": args.temperatures,
                "models": args.model_paths,
            }
        )

if __name__ == "__main__":
    try:
        main()
    finally:
        cleanup_distributed()
